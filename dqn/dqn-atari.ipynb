{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0r-iGVrY_dPh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-01-20T22:57:31.834210Z",
     "iopub.status.busy": "2024-01-20T22:57:31.833670Z",
     "iopub.status.idle": "2024-01-20T22:57:38.873441Z",
     "shell.execute_reply": "2024-01-20T22:57:38.872853Z",
     "shell.execute_reply.started": "2024-01-20T22:57:31.834184Z"
    },
    "executionInfo": {
     "elapsed": 18747,
     "status": "ok",
     "timestamp": 1705481744437,
     "user": {
      "displayName": "Gareth Davies",
      "userId": "11833826550098318581"
     },
     "user_tz": 0
    },
    "id": "0r-iGVrY_dPh",
    "outputId": "11308d3e-2a19-4bea-c9e2-c7249b4c4475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.29.1)\n",
      "Requirement already satisfied: wandb>0.16.0 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.16.2)\n",
      "Requirement already satisfied: matplotlib in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (3.8.2)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.16.2-cp310-cp310-macosx_10_13_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.3.0 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from gymnasium->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from gymnasium->-r requirements.txt (line 1)) (1.26.3)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from gymnasium->-r requirements.txt (line 1)) (0.0.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from gymnasium->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: shimmy[atari]<1.0,>=0.1.0 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from gymnasium->-r requirements.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from gymnasium->-r requirements.txt (line 1)) (0.4.2)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from wandb>0.16.0->-r requirements.txt (line 4)) (3.1.41)\n",
      "Requirement already satisfied: setproctitle in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from wandb>0.16.0->-r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from wandb>0.16.0->-r requirements.txt (line 4)) (8.1.7)\n",
      "Requirement already satisfied: PyYAML in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from wandb>0.16.0->-r requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: setuptools in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from wandb>0.16.0->-r requirements.txt (line 4)) (63.2.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from wandb>0.16.0->-r requirements.txt (line 4)) (5.9.8)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from wandb>0.16.0->-r requirements.txt (line 4)) (1.39.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from wandb>0.16.0->-r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from wandb>0.16.0->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from wandb>0.16.0->-r requirements.txt (line 4)) (4.25.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from wandb>0.16.0->-r requirements.txt (line 4)) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (3.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (23.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (4.47.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: torch==2.1.2 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from torchvision->-r requirements.txt (line 6)) (2.1.2)\n",
      "Requirement already satisfied: jinja2 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from torch==2.1.2->torchvision->-r requirements.txt (line 6)) (3.1.3)\n",
      "Requirement already satisfied: sympy in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from torch==2.1.2->torchvision->-r requirements.txt (line 6)) (1.12)\n",
      "Requirement already satisfied: filelock in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from torch==2.1.2->torchvision->-r requirements.txt (line 6)) (3.13.1)\n",
      "Requirement already satisfied: networkx in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from torch==2.1.2->torchvision->-r requirements.txt (line 6)) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from torch==2.1.2->torchvision->-r requirements.txt (line 6)) (2023.12.2)\n",
      "Requirement already satisfied: tqdm in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium->-r requirements.txt (line 1)) (4.66.1)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium->-r requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb>0.16.0->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb>0.16.0->-r requirements.txt (line 4)) (4.0.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb>0.16.0->-r requirements.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb>0.16.0->-r requirements.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb>0.16.0->-r requirements.txt (line 4)) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb>0.16.0->-r requirements.txt (line 4)) (3.6)\n",
      "Requirement already satisfied: ale-py~=0.8.1 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from shimmy[atari]<1.0,>=0.1.0->gymnasium->-r requirements.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: importlib-resources in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium->-r requirements.txt (line 1)) (6.1.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>0.16.0->-r requirements.txt (line 4)) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from jinja2->torch==2.1.2->torchvision->-r requirements.txt (line 6)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/garethdavies/Development/workspaces/rl/venv/lib/python3.10/site-packages (from sympy->torch==2.1.2->torchvision->-r requirements.txt (line 6)) (1.3.0)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.16.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "376173c5",
   "metadata": {
    "executionInfo": {
     "elapsed": 10879,
     "status": "ok",
     "timestamp": 1705481763504,
     "user": {
      "displayName": "Gareth Davies",
      "userId": "11833826550098318581"
     },
     "user_tz": 0
    },
    "id": "376173c5"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import pickle\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0ae1f72",
   "metadata": {
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1705481771579,
     "user": {
      "displayName": "Gareth Davies",
      "userId": "11833826550098318581"
     },
     "user_tz": 0
    },
    "id": "a0ae1f72"
   },
   "outputs": [],
   "source": [
    "environment = \"ALE/Breakout-v5\"\n",
    "bs = 32\n",
    "replay_memory_size= 400_000 #replay memory max size\n",
    "sync_every_n_steps = 10_000\n",
    "gamma = 0.99\n",
    "lr=0.0000625\n",
    "epsilon = 1\n",
    "final_epsilon = 0.1,\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "Optimizer = torch.optim.Adam\n",
    "replay_start_size = 50_000\n",
    "FRAMES_TO_TRAIN = 10_000_000\n",
    "EPOCH_SIZE = 50_000\n",
    "BASE_MODEL = None\n",
    "final_exploration_frame = 1_000_000\n",
    "MIN_STACK_SIZE = 4\n",
    "max_grad_norm = 10\n",
    "adam_epsilon = 1.5e-4\n",
    "replay_period = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7888d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"DQN-Deepmind-2015\",\n",
    "    \"environment\": environment,\n",
    "    \"epsilon\": epsilon,\n",
    "    \"final_epsilon\": final_epsilon,\n",
    "    \"gamma\":gamma,\n",
    "    \"bs\":bs,\n",
    "    \"replay_memory_size\":replay_memory_size,\n",
    "    \"sync_every_n_steps\": sync_every_n_steps,\n",
    "    \"loss\": str(loss_fn),\n",
    "    \"Optimizer\": Optimizer.__name__,\n",
    "    \"frames_to_train\": FRAMES_TO_TRAIN,\n",
    "    \"epoch_size\":EPOCH_SIZE,\n",
    "    \"base_model\": BASE_MODEL,\n",
    "    \"final_exploration_frame\":final_exploration_frame,\n",
    "    \"replay_start_size\":replay_start_size,\n",
    "    \"max_grad_norm\":max_grad_norm,\n",
    "    \"adam_epsilon\":adam_epsilon,\n",
    "    \"replay_period\":replay_period\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dc4ad55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T23:00:55.330097Z",
     "iopub.status.busy": "2024-01-20T23:00:55.329165Z",
     "iopub.status.idle": "2024-01-20T23:00:59.771287Z",
     "shell.execute_reply": "2024-01-20T23:00:59.770703Z",
     "shell.execute_reply.started": "2024-01-20T23:00:55.330065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ft4pmrub) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a397cd3349a4595ba518990878edfd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peachy-shadow-17</strong> at: <a href='https://wandb.ai/garethmd/atari/runs/ft4pmrub' target=\"_blank\">https://wandb.ai/garethmd/atari/runs/ft4pmrub</a><br/> View job at <a href='https://wandb.ai/garethmd/atari/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzMTkzNjA0NQ==/version_details/v1' target=\"_blank\">https://wandb.ai/garethmd/atari/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzMTkzNjA0NQ==/version_details/v1</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240120_230052-ft4pmrub/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ft4pmrub). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1560c095b2c64064a6c0350564e4322c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113193878231363, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20240120_230055-gswyd628</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garethmd/atari/runs/gswyd628' target=\"_blank\">quiet-snow-18</a></strong> to <a href='https://wandb.ai/garethmd/atari' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garethmd/atari' target=\"_blank\">https://wandb.ai/garethmd/atari</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garethmd/atari/runs/gswyd628' target=\"_blank\">https://wandb.ai/garethmd/atari/runs/gswyd628</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/garethmd/atari/runs/gswyd628?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5e7a616f70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"atari\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f900d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1705481765159,
     "user": {
      "displayName": "Gareth Davies",
      "userId": "11833826550098318581"
     },
     "user_tz": 0
    },
    "id": "a4f900d0",
    "outputId": "dd9ec3fb-d6b1-4751-f00f-7ee107ee89fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ae8f22d",
   "metadata": {
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1705481767156,
     "user": {
      "displayName": "Gareth Davies",
      "userId": "11833826550098318581"
     },
     "user_tz": 0
    },
    "id": "2ae8f22d"
   },
   "outputs": [],
   "source": [
    "class TorchEnv:\n",
    "    def __init__(self, env: gym.Env, transforms=None, terminate_after_lose_life=True, image_listener=None):\n",
    "        self.env = env\n",
    "        self.n_observations = self.env.observation_space.shape[0]\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        self.transforms = transforms\n",
    "        self.terminate_after_lose_life = terminate_after_lose_life\n",
    "        self.image_listener = image_listener\n",
    "        \n",
    "\n",
    "    def step(self, a):\n",
    "        s, r, terminated, _, info = self.env.step(a)\n",
    "        r = np.clip(r, -1, 1) # clip rewards between -1 and 1 as per DeepMind\n",
    "\n",
    "        if self.image_listener:\n",
    "            self.image_listener(s)\n",
    "\n",
    "        # terminate if we lose a life as per Rainbow\n",
    "        # https://arxiv.org/pdf/1710.02298.pdf\n",
    "        if self.lives != info['lives'] and self.terminate_after_lose_life:\n",
    "            terminated = True\n",
    "        \n",
    "        if self.transforms:\n",
    "            s = s[None, :, :, :] # HWC -> BCHW\n",
    "            s = self.transforms(s)\n",
    "        return s, r, terminated\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        s, info = self.env.reset(*args, **kwargs)\n",
    "        if self.image_listener:\n",
    "            self.image_listener(s)\n",
    "        self.lives = info['lives']\n",
    "    \n",
    "        # at episode start, repeat and stack the first state 4 times\n",
    "        stack = [s]\n",
    "        stack = np.vstack([s[None, :, :, :]] * MIN_STACK_SIZE) # HWC -> BHWC\n",
    "        \n",
    "        if self.transforms:\n",
    "            stack = self.transforms(stack)\n",
    "        \n",
    "        return stack\n",
    "\n",
    "    def close(self):\n",
    "        return self.env.close()\n",
    "    \n",
    "    def sample(self):\n",
    "        return np.random.randint(self.n_actions)\n",
    "\n",
    "env = TorchEnv(gym.make(environment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ba7ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageListener:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.images = []\n",
    "\n",
    "    def __call__(self, img):\n",
    "        self.images.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83317d5b",
   "metadata": {
    "id": "83317d5b"
   },
   "source": [
    "## Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b7d9f1",
   "metadata": {
    "id": "b3b7d9f1"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23fe59a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13254d030>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = TorchEnv(gym.make(environment))\n",
    "stack = env.reset() \n",
    "plt.imshow(stack[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d079db6b",
   "metadata": {},
   "source": [
    "#### Greyscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c01eb3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGrayscale:\n",
    "    def __init__(self):\n",
    "        self.transforms = transforms.Grayscale()\n",
    "\n",
    "    def __call__(self, batch: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch (torch.Tensor): expects batch to be of shape (B, H, W, C)\n",
    "        Returns:\n",
    "            torch.Tensor: batch of shape (B, C, H, W)\n",
    "        \"\"\"\n",
    "        grey_pt_stack = torch.stack([self.transforms(s.permute(2,0,1)) for s in batch])\n",
    "        return grey_pt_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f194f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imglistshow(v_slice):\n",
    "    plt.figure()\n",
    "    #subplot(r,c) provide the no. of rows and columns\n",
    "    f, axarr = plt.subplots(1,4) \n",
    "\n",
    "    # use the created array to output your multiple images. In this case I have stacked 4 images vertically\n",
    "    axarr[0].imshow(v_slice[0])\n",
    "    axarr[1].imshow(v_slice[1])\n",
    "    axarr[2].imshow(v_slice[2])\n",
    "    axarr[3].imshow(v_slice[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "152a968b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "greyscale = BatchGrayscale() #transforms to grayscale\n",
    "pt_stack = torch.from_numpy(stack).to(device)\n",
    "img_stack = greyscale(pt_stack)\n",
    "imglistshow(img_stack.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "557d7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchDownsample:\n",
    "    def __init__(self, size=(110,84)):\n",
    "        self.size = size\n",
    "    \n",
    "    def __call__(self, batch: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch (torch.Tensor): expects batch to be of shape (B, C, H, W)\n",
    "        Returns:\n",
    "            torch.Tensor: batch of shape (B, C, H, W)\n",
    "        \"\"\"\n",
    "        orig_dtype = batch.dtype\n",
    "        result = nn.functional.interpolate(batch.float(), size=self.size,  mode='bilinear')\n",
    "        return result.type(orig_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3659ff7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAC9CAYAAACZOYZcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAfzElEQVR4nO3dfXRU9b3v8fdMkpkkkAeSQB4gPCpCwEAXDzG3iLREKbReEWp9oC2KhYUlrkrsqictBbHtidd6W44WtedYob3CUfH6cLU1PQUqHjzBh3AoxwNSAlEQSCJiMpOQx5l9/4gZMoZEkpnJ3jPzea01i+zf/s3ev5l81o9v9uzZ22YYhoGIiIiIhdjNHoCIiIjI56lAEREREctRgSIiIiKWowJFRERELEcFioiIiFiOChQRERGxHBUoIiIiYjkqUERERMRyVKCIiIiI5ahAEREREcsxtUDZvHkzY8eOJT4+noKCAt5++20zhyMmUAZEGRBQDqQn0wqUZ599lpKSEjZs2MD+/fuZNm0aCxYsoK6uzqwhySBTBkQZEFAO5OJsZt0ssKCggFmzZvGb3/wGAK/XS25uLnfffTf/8A//0OdzvV4vp0+fJikpCZvNNhjDlQAYhoHb7SYnJwe7/UJNrAxEj1BkoKu/chA+NBdIbxm4mNhBGpOftrY2KisrKS0t9bXZ7XaKioqoqKjo0b+1tZXW1lbf8qlTp8jLyxuUsUrwnDx5klGjRgHKQLQKJAOgHEQKzQXSPQO9MaVAOXv2LB6Ph8zMTL/2zMxM3n///R79y8rK2LhxY4/2OSwilriQjDEmIwOSh/S63nviFEZHBzank5is4RgxMRftZ2trxzjfjOfcp2CPITYnE8PR+5g9xz/s30BtNmKGpWJLScLo5a8Hm8dDx4cf9W+73cSkDcOWmABxsRgNbjyf1kM/Drx10M5e/kRSUpKvTRlQBvqbAVAO+hQlOVAG+hCmGeiNKQVKf5WWllJSUuJbdrlc5ObmEkscsbbQBNKTP4FPL3cCYG/vbDPsYHyWuxH/6sbrdhObPoJPrh5LR6INvGAzAKOzL3ZwNhgMOdlMbMXfsCcOwX3VeJrT7dgMsHV8tt2Yzv42D6R/WAtezyWP0+Z0YlwxnnOTh2DE0jkGr3+fuPMGSSdqB/xeGJMn0DgqgbYhNtIONxFX+T5Ge1s/NvDZWAM4/KoM9C5aMgDKQV+iJQfKQO8iLQOmFCgZGRnExMRQW+v/JtXW1pKVldWjv9PpxOl0DtbwADj2HTt7in6Jx4DfnJ1LuxHDtCEnuXZIFQCr//pdcLtpvnIUt//kFRYNPcK/N4+hqiWTRo+TGUM+YFb8Sf7p46/wp9dnMKECbNkjGP/Dw/x85J9we2P47dm5ABSl/DfTnXWc88RRuu0avC2XHkh70lCOLh7CEzf9M6n2Zo61D2eva6JfnwOfjIJnB/5eHF+awDfmvcuilL+x5vnvcfnRoXg+OTfwDaIMKAP9zwAoB32JlhwoA70L1wz0xpQCxeFwMGPGDHbt2sXixYuBzhOddu3aRXFxsRlD6pXbiOXwdy7D1tLGn++cybXf+WWvfde/dhM5ewyGnGjk/66YzdaFv+21777mcRz71kgAyn+Rx84vbw54rK+583lq/5e57Lcdfu2JbR0Ecia0EYLzzpQBZSCcMgDKASgHykBoMtAb0z7iKSkpYfny5cycOZPZs2ezadMmmpqauOOOO8wa0kV5DRvG8RN4WlqIc+X02TfWbSOhphn7hzXEuCfSblz8c0iAFiOOjuMfANDePDMoY81POMHXpqTy2qopfu32+gQuqwzKLoJKGVAGwiUDoByEUrjkQBkYXKYVKDfffDMff/wx69evp6amhunTp1NeXt7jRCn5Ah0dJJ6x8eAHixge3whA4cTjJMW18N2MN8mNPc8rjZP5f6SbPNCelIEgUQYElAMJ6wxcjKknyRYXF1vuEN7F2OKd2DzezhOd+mDEgBFrx+Z0gh1ivuBAmi3O0fmvfeAH3Dz1DWQ9+ha2x2LwfQpot1E/YQxPPnUNG3NeG/C2B4MyoAyESwZAOQilcMmBMjB4wuJbPGZKi2ln/f7deLCRZH+jz74v3vYrmm6NxWvYSbK3kWTv/eSmRUMOM/1I51fI0uyvD3h8MRnpHPvBRH5y0w4mOC5cdTHOtpd0e2sfz5RLpQwIKAeiDAw2064kGwiXy0VKSgrzuCFkXyvjqnzcYxN7XZ3yyn/hbWoiZvhwGv/HODri7XCRk4ccLg8Jp5vwHjiEPTGRlqvzaB128brQ5jEY+vzb/fpOuS3OgS1vAu7Lk/HGXvzspdhmLwkvD/y+FrYvTaElO5GOBDtJVS6MQ8f69bWyDqOd13mZhoYGkpOTBzyO7pSBbs+J0gyAcuD3nCjNgTLQ7TkRloGwLlCmrPxHYhzxZg9HvoCnrYX//pcfh2RSUgbCQygyAMpBuNFcIP3JQFgXKAcPjSApydQbMsslcLu95OfVhWRSUgbCQygyAMpBuNFcIP3JgH6bIiIiYjkqUERERMRyVKCIiIiI5ahAEREREctRgSIiIiKWowJFRERELEcFioiIiFiOChQRERGxnIi6F8/x9mTa+riltYSew+ZhfJzLtP0rA+YzOwOgHFiB2TlQBswXaAYiqkB56BtL8Rw5bvYwolrMpAk8Uf6UaftXBsxndgZAObACs3OgDJgv0AxEVIFChwe8vd8xUgaBx2vu/pUB85mdAVAOrMDsHCgD5gswAzoHRURERCxHBYqIiIhYjgoUERERsRwVKCIiImI5KlBERETEclSgiIiIiOUEvUApKytj1qxZJCUlMWLECBYvXsyRI0f8+sybNw+bzeb3WL16dbCHIiZ57DeN3PD1s0ydVMvM6XX8oLi+Rx9lIPJ1z8E1cz4G4OjRo359lIPIprlAAhH066Ds2bOHNWvWMGvWLDo6Ovjxj3/Mddddx6FDhxgyZIiv38qVK3nggQd8y4mJicEeipjkrX1tfGd5IvnT4ujwwIO/cAPQ1NREcnKyr58yENm658Dl8nLTkk+58cYbOXz4sOaCKKG5QAIR9AKlvLzcb3nr1q2MGDGCyspK5s6d62tPTEwkKysr2LsXC/j902l+yz/7x2TmzTnLgQMHyM7O9rUrA5Gtew7c7s4LNp08eVJzQRTRXCCBCPk5KA0NDQCkpfkHddu2bWRkZDB16lRKS0s5f/58r9tobW3F5XL5PSR8NH72n9OwYcP82pWB6KS5IHppLpD+COml7r1eL/fccw9f/vKXmTp1qq/9tttuY8yYMeTk5HDw4EHuu+8+jhw5wgsvvHDR7ZSVlbFx48ZQDlVCxOs1eOjBRgDy8vJ87cpAdPF6DQCuuuoqzQVRSnOB9FdIC5Q1a9bw3nvvsXfvXr/2VatW+X6+8soryc7OZv78+Rw7dowJEyb02E5paSklJSW+ZZfLRW5ubugGLkGz/icuqo529GhXBqLLL37W+R/TU0/53zhMOYgemgukv0L2EU9xcTGvvvoqf/3rXxk1alSffQsKCgCoqqq66Hqn00lycrLfQ6xv/ToXu3e18uTWYV/YVxmIXOvXuXhjTysAI0eO7LOvchCZNBfIQAS9QDEMg+LiYl588UV2797NuHHjvvA5Bw4cAPA7aUrCl2EYrF/n4t/KW9j2bBqjRsV84XOUgcjTPQdPPpV6Sc9RDiKL5gIJRNA/4lmzZg3bt2/n5ZdfJikpiZqaGgBSUlJISEjg2LFjbN++nUWLFpGens7BgwdZu3Ytc+fOJT8/P6B9N0wfTmKmqmkzuUfGs/4nLl5+uYV/fnIYQ4fYOPtx5y3Pm5ubSU5OVgYinHtkPIBfDoYMsQFQW1tLXFyc5oIooLlAuuaCgQp6gfL4448DnRff6W7Lli3cfvvtOBwOdu7cyaZNm2hqaiI3N5elS5eybt26gPc97Ud/w9Ue2BsigUmOa+G3M5sBuPVb5/zWvfDCC9x1113KQIRLjmsB4On/0zMHEydO1FwQJTQXSNdcMFBBL1AMw+hzfW5uLnv27An2bsVCqk/6X8/A7faSn1fHsmXLAGUgWnTPQVcGGhoafOcMKAeRT3OBBEL34hERERHLCenXjAdbjrOepFgd0jNTUkxgh/QCpQyYz+wMgHJgBWbnQBkwX6AZiKgCJT/hBO1GRL2ksBNn63mdg8GkDJjP7AyAcmAFZudAGTBfoBnQRzwiIiJiOSpQRERExHIi6vhXjM2LF6/Zw4hqMTZz339lwHxmZ6BrDMqBuczOgTJgvkAzEFFHUGLo+yvOEnpm/w7M3r9Y43dghTFEO7N/B2bvXwL/HURUgSIiIiKRQQWKiIiIWE5EnYOS5/jE7CGIyZQBAeVAlIFIEFEFSmaME7sOCpnKi5daT6tp+1cGzGd2BkA5sAKzc6AMmC/QDERUgRJLDDE2BdJMHsNm6v6VAfOZnQFQDqzA7BwoA+YLNAP67YmIiIjlRNQRlMo2D94vuJuyhJbd5iUrxrz9KwPmMzsDoBxYgdk5UAbMF2gGIqpAOdmeTosRZ/Ywolq8rZ2smNOm7V8ZMJ/ZGQDlwArMzoEyYL5AMxBRBconnqGc9zrMHkZUS7S3mbp/ZcB8ZmcAlAMrMDsHyoD5As2AzkERERERy4moIyiV7jE0tCeYPYyolhLXzNeG/N20/SsD5jM7A6AcWIHZOVAGzBdoBiKqQPnPj0fR1KJDemZKSmiFLPP2rwyYz+wMgHJgBWbnQBkwX6AZiKgCpaExgbbzOinKTB6PuZ8aKgPmMzsDoBxYgdk5UAbMF2gGIqpAMQzAAheJimaGye+/MmA+szPQOQaUA5OZnQNlwHyBZiDoBcr999/Pxo0b/dquuOIK3n//fQBaWlq49957eeaZZ2htbWXBggU89thjZGZmBrzv9vMObI0mX4AhyrXZ4tj0Kzf/9OumXvsoA5Gtzdb5V+vFcjBz5kz+/vfOz6SVg8imuUC65oKBCskRlClTprBz584LO4m9sJu1a9fyxz/+kR07dpCSkkJxcTFLlizhzTffDHi/47cZOE98HPB2ZOBaxqVBHkycGMvT/zoMgMZGL1+95sKNu5SByNYyLg3md/7clYOuDPz5z3/29VMOIpvmAuk+FwxESAqU2NhYsrJ6nhnT0NDA7373O7Zv385Xv/pVALZs2cLkyZPZt28fV111VUD7dZ74FM/R4wFtQwITb7dDHsTEwvARnX+9xCdcOMynDES+ePuFz527ctCVgfT0dEA5iAaaC6T7XDAQISlQjh49Sk5ODvHx8RQWFlJWVsbo0aOprKykvb2doqIiX99JkyYxevRoKioqeg1ka2srra0X7ojocrlCMWwJog+qPRTMqMMZb+PKKy/ETBmILl05iPvsyxQnT55kypQpykEU0VwgAxX006wLCgrYunUr5eXlPP7441RXV3P11VfjdrupqanB4XCQmprq95zMzExqamp63WZZWRkpKSm+R25ubrCHLUE0/UsOfvmrFLY+PYyf/SKZU6c8AMpAlOmeg3XrkwBYuHChchBFNBdIIIJ+BGXhwoW+n/Pz8ykoKGDMmDE899xzJCQM7KI5paWllJSU+JZdLpdCaWHzvuL0/Tx5Mlx2eQxzrjrLiy++SFpa2oC2qQyEn+45GDWq8xB/Q0OD5oIoorlAAhHyrxmnpqYyceJEqqqquPbaa2lra6O+vt6vaq6trb3oOStdnE4nTqez1/VibcnJnQfqjh8/Tl5enjIQxSZMmKC5IIppLpD+CPmVdBobGzl27BjZ2dnMmDGDuLg4du3a5Vt/5MgRTpw4QWFhYaiHIiY53+QFICsrSxmIctXV1ZoLopjmAumPoB9B+eEPf8j111/PmDFjOH36NBs2bCAmJoZbb72VlJQU7rzzTkpKSkhLSyM5OZm7776bwsLCgM/YFuv4xc9czC+KZ9QoO7W1Xh5+yA3AN7/5TWUginTPQXV157kHmguii+YCCUTQC5SPPvqIW2+9lU8++YThw4czZ84c9u3bx/DhwwH49a9/jd1uZ+nSpX4X5pHIUXPGyw+K66mv95KWZmf6lzov1pORkQEoA9Giew5Sh3UerN25c6fmgiiiuUACEfQC5ZlnnulzfXx8PJs3b2bz5s3B3rVYxKOPpfotu91eyl+r8y0rA9Ghew7cbi/5eXWMHz/e16YcRD7NBRII8+/qJSIiIvI5KlBERETEclSgiIiIiOWoQBERERHLUYEiIiIilqMCRURERCxHBYqIiIhYjgoUERERsRwVKCIiImI5KlBERETEclSgiIiIiOWoQBERERHLUYEiIiIilqMCRURERCxHBYqIiIhYjgoUERERsRwVKCIiImI5KlBERETEclSgiIiIiOWoQBERERHLUYEiIiIilhP0AmXs2LHYbLYejzVr1gAwb968HutWr14d7GGIieYU1jEut8b3yM+rA+Dee+8FlIFo0T0HXRlISUnRXBBFNBdIIGKDvcF33nkHj8fjW37vvfe49tpruemmm3xtK1eu5IEHHvAtJyYmBnsYYqKXX83A6zF8ywf+s51V36tn8eLFvjZlIPJ1z0Fjo5evXvMJgOaCKKK5QAIR9AJl+PDhfssPPvggEyZM4JprrvG1JSYmkpWVFexdi0Wkp/sfmNuzpxGAOXPm+NqUgcjXPQfxCTYAxo0bp7kgimgukECE9ByUtrY2nn76aVasWIHNZvO1b9u2jYyMDKZOnUppaSnnz5/vczutra24XC6/h4SHtjaDP77SAqAMRLH2ts6/or/97W8rB1FKc4H0V9CPoHT30ksvUV9fz+233+5ru+222xgzZgw5OTkcPHiQ++67jyNHjvDCCy/0up2ysjI2btwYyqFKiPzbn1twuw2/NmUg+uze1QrAsmXLfG3KQXTRXCD9ZTMMw/jibgOzYMECHA4Hr7zySq99du/ezfz586mqqmLChAkX7dPa2kpra6tv2eVykZuby8FDI0hKunAQaPX87+I5ejx4L0D6LeaKy3jiL1t9y99ddg6bHd54vY2GhgaSk5N7PEcZiCyfzwDAslvO8R9v9p4BUA4ijeYCudhc4HZ7yc+r63Mu6BKyj3g+/PBDdu7cyfe+970++xUUFABQVVXVax+n00lycrLfQ6zvo488vLm3jaVLE/rspwxEto8+8rCvou0L+ykHkUtzgQxEyAqULVu2MGLECL7+9a/32e/AgQMAZGdnh2ooYpLnnztPeoadq69x9NlPGYhszz93nrS0L55qlIPIpblABiIk56B4vV62bNnC8uXLiY29sItjx46xfft2Fi1aRHp6OgcPHmTt2rXMnTuX/Pz8UAxFTOL1Gux4rpml30wgNvbCCXHKQHTpysH/XBzPU09eOPlROYgemgtkoEJSoOzcuZMTJ06wYsUKv3aHw8HOnTvZtGkTTU1N5ObmsnTpUtatWxeKYYiJ9v57G6dPebnpZv9DuspAdOnKweIl/gWKchA9NBfIQIWkQLnuuuu42Lm3ubm57NmzJxS7FIuZe42T6pOd1zZwu72+dmUgunTloHsGQDmIJpoLZKB0Lx4RERGxHBUoIiIiYjkqUERERMRyVKCIiIiI5ahAEREREctRgSIiIiKWowJFRERELEcFioiIiFiOChQRERGxHBUoIiIiYjkqUERERMRyVKCIiIiI5ahAEREREctRgSIiIiKWowJFRERELEcFioiIiFiOChQRERGxHBUoIiIiYjkqUERERMRyVKCIiIiI5ahAEREREcvpd4HyxhtvcP3115OTk4PNZuOll17yW28YBuvXryc7O5uEhASKioo4evSoX59z586xbNkykpOTSU1N5c4776SxsTGgFyKD51PjYw4Yb/KG8So7jeepM075rTcMg1897Gb2jDomXVbDyhWf9tiGMhDelAEB5UBCK7a/T2hqamLatGmsWLGCJUuW9Fj/0EMP8cgjj/D73/+ecePG8dOf/pQFCxZw6NAh4uPjAVi2bBlnzpzhL3/5C+3t7dxxxx2sWrWK7du392ss57127N7oPghki3Ngnzju0jqfqsHb2ITR0RHQPj10MJQUchjLQSr8VxoGD29u5Q9Pneen/zuDnNxYHvtfnwLttLS0kJycDCgDwRTtGQDlAJQDZcCcDPTJMKjxOP2amjzeS356vwuUhQsXsnDhwl7GYrBp0ybWrVvHDTfcAMAf/vAHMjMzeemll7jllls4fPgw5eXlvPPOO8ycOROARx99lEWLFvHwww+Tk5NzyWPZ3zqKhLgLL8HW4envywl7MSOzGPYvtZfU98NfTiL57ZN0nDod0D4zbNlkkN25YHxuZWsbTz3ZyqTlszhwZT4HgBH3NMLrz/Dqq6+yYsUKZSDIoj0DoByAcqAMmJOBvtja2nnkzLV+be1NbcAfLun5QS03q6urqampoaioyNeWkpJCQUEBFRWd1XVFRQWpqam+MAIUFRVht9t56623Lrrd1tZWXC6X30OsqbmjgZZPmsmaNdLXFjfUAcA777wDKAORLlQZAOUgnGgukED1+whKX2pqagDIzMz0a8/MzPStq6mpYcSIEf6DiI0lLS3N1+fzysrK2LhxYzCHGjG8H3/CB7+aekl9U/afxvtpfUjH0+o5D0B8WkKPdbW1nZW9MhBc0ZIBUA76Ei05UAZ6Z7UMBCqoBUqolJaWUlJS4lt2uVzk5uZysi2d+La4Cx2Nzx9jjHzepiaGPN/7X5zdhfCTRh+jtRWAdw5OIDal8zNmb3NLwNtVBnoXLRkA5aAv0ZIDZaB3lstAcwv/UXmFX1t/MhDUAiUrKwvorI6zs7N97bW1tUyfPt3Xp66uzu95HR0dnDt3zvf8z3M6nTidzh7tr52aQkzihfbh7U2BvgQJUOy5ZgBG/aSCJFsqAB1GOye5cGRNGYhsocoAKAfhRHOBeGrruPzuz/1+P8vApQhqgTJu3DiysrLYtWuXryBxuVy89dZb3HXXXQAUFhZSX19PZWUlM2bMAGD37t14vV4KCgr6tb/Ub31ArO1Cxdzhjb6ToqwmgSE4iOccdSSRCkAH7QDMmjULUAYi3WBnAJQDK9JcIIHqd4HS2NhIVVWVb7m6upoDBw6QlpbG6NGjueeee/j5z3/O5Zdf7vuacU5ODosXLwZg8uTJfO1rX2PlypU88cQTtLe3U1xczC233NLvM/fxesAW3V8rM0OH0UEzF65T0EwTbqOeOBzE2xIZbVxGNYdJNIaSwBCO8l8AfOMb3wCUgUhgqQyAcmASS+VAGYg4/S5Q3n33Xb7yla/4lrs+C1y+fDlbt27lRz/6EU1NTaxatYr6+nrmzJlDeXm57xooANu2baO4uJj58+djt9tZunQpjzzySBBejgwGF+fYzxu+5aMcBCCbMUxhFmO4Ag8eDlNJB+0kkwagDEQQZUBAOZDQshlG+J1J5HK5SElJYR43+B3SE2vqMNp5nZdpaGjwXZwpUMpAeAlFBkA5CDeaC6Q/GdDxMBEREbEcFSgiIiJiOSpQRERExHJUoIiIiIjlhMWVZD+v67zeDtp73qBKLKfr2gfBPB9bGQgvochA9+0pB+FBc4H0JwNhWaC43W4A9vInk0ci/eF2u0lJSQnatkAZCDfBzEDX9kA5CDeaC+RSMhCWXzP2er0cOXKEvLw8Tp48GdSvLYYL3/0nwuD1G4aB2+0mJycHuz04nyoqA8oAKAegHCgDkZuBsDyCYrfbGTmy8xbeycnJlv+FhFK4vP5g/tUMykB34fL6g50BUA66C5fXr7kgdMLl9V9qBnSSrIiIiFiOChQRERGxnLAtUJxOJxs2bLjobbejQbS/ftB7EO2vv0u0vw/R/vpB70Gkvv6wPElWREREIlvYHkERERGRyKUCRURERCxHBYqIiIhYjgoUERERsRwVKCIiImI5YVmgbN68mbFjxxIfH09BQQFvv/222UMKifvvvx+bzeb3mDRpkm99S0sLa9asIT09naFDh7J06VJqa2tNHPHgUg46RXMOlIFOyoAyEIkZCLsC5dlnn6WkpIQNGzawf/9+pk2bxoIFC6irqzN7aCExZcoUzpw543vs3bvXt27t2rW88sor7Nixgz179nD69GmWLFli4mgHj3KgHCgDyoAyEOEZMMLM7NmzjTVr1viWPR6PkZOTY5SVlZk4qtDYsGGDMW3atIuuq6+vN+Li4owdO3b42g4fPmwARkVFxSCN0DzKQadozoEy0EkZUAYiNQNhdQSlra2NyspKioqKfG12u52ioiIqKipMHFnoHD16lJycHMaPH8+yZcs4ceIEAJWVlbS3t/u9F5MmTWL06NER+150UQ6UA2VAGVAGIj8DYVWgnD17Fo/HQ2Zmpl97ZmYmNTU1Jo0qdAoKCti6dSvl5eU8/vjjVFdXc/XVV+N2u6mpqcHhcJCamur3nEh9L7pTDpQDZUAZUAYiPwOxZg9Aerdw4ULfz/n5+RQUFDBmzBiee+45EhISTByZDCblQJQBicYMhNURlIyMDGJiYnqcmVxbW0tWVpZJoxo8qampTJw4kaqqKrKysmhra6O+vt6vTzS8F8qBcqAMKAPKQORnIKwKFIfDwYwZM9i1a5evzev1smvXLgoLC00c2eBobGzk2LFjZGdnM2PGDOLi4vzeiyNHjnDixImIfy+UA+VAGVAGlIEoyIDZZ+n21zPPPGM4nU5j69atxqFDh4xVq1YZqampRk1NjdlDC7p7773XeP31143q6mrjzTffNIqKioyMjAyjrq7OMAzDWL16tTF69Ghj9+7dxrvvvmsUFhYahYWFJo96cCgHyoEyoAwoA5GdgbArUAzDMB599FFj9OjRhsPhMGbPnm3s27fP7CGFxM0332xkZ2cbDofDGDlypHHzzTcbVVVVvvXNzc3G97//fWPYsGFGYmKiceONNxpnzpwxccSDSznoFM05UAY6KQPKQCRmwGYYhmH2URwRERGR7sLqHBQRERGJDipQRERExHJUoIiIiIjlqEARERERy1GBIiIiIpajAkVEREQsRwWKiIiIWI4KFBEREbEcFSgiIiJiOSpQRERExHJUoIiIiIjl/H9AAWPQmTzslgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_downsample = BatchDownsample(size=(110,84))\n",
    "downsampled_stack = batch_downsample(img_stack)\n",
    "imglistshow(downsampled_stack.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dc85b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCrop:\n",
    "    def __init__(self, size=(84,84)):\n",
    "        self.size = size\n",
    "    \n",
    "    def __call__(self, batch: torch.Tensor):\n",
    "        \"\"\" crop 17 pixels from top and bottom\n",
    "        Args:\n",
    "            batch (torch.Tensor): expects batch to be of shape (B, C, H, W)\n",
    "        Returns:\n",
    "            torch.Tensor: batch of shape (B, C, H, W)\n",
    "        \"\"\"\n",
    "        return batch[:, :,  17:17+self.size[0], :] # crop 17 pixels from top and bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fb34dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAACcCAYAAADf5smOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAP+0lEQVR4nO3dXWxTZ57H8Z9tYic0JCEJkxfywsAMk4YZYDYU16si7XbSRYzUi0IlhHpRVZWqqoBWpL0o0qqhGo0i7a6mldqUq6pc7Fa8SFutuFgqNVWR6AaxDZq9aYt4yWwCiZ1SbYh5iWPiZy8yMckCI4Lt5zn2+X6kSPjYiR8ff3X4c+zggDHGCAAAwJKg6wUAAAB/YfgAAABWMXwAAACrGD4AAIBVDB8AAMAqhg8AAGAVwwcAALCK4QMAAFjF8AEAAKxi+AAAAFYVbPjo7+/XmjVrVF5ermg0qnPnzhXqruBRNAAagEQHuF9Bho9jx46pp6dHvb29On/+vDZt2qTt27drYmKiEHcHD6IB0AAkOsCDBQrxwXLRaFRPPfWUPvzwQ0lSJpNRa2ur9u/fr7fffvsvfm8mk9HY2JhWrFihQCCQ76Uhz4wxSiaTam5uVjB4b5bNpYH529NBcaABSIXpgAaKy8MaeJBl+b7zmZkZDQ0N6eDBg9ltwWBQ3d3dGhwcvO/2qVRKqVQqe/natWvq7OzM97JQYKOjo2ppaZG09AYkOigFNAAptw5ooDQsbOBh8j58XL9+XbOzs2poaFi0vaGhQd9///19t+/r69O777573/b/PFevysp7k9OB5/do9vKf8r1cLEHo52v13r//66JtN29m9Ndbr2vFihXZbUttQKKDYvL/O6AB/ynUsYAGisejNvAweR8+lurgwYPq6enJXp6amlJra6sqK4NaseJebMtCEQUCZS6WiD8LhSKLnpOFcj0lSgfF42Ed0IB/FOpYQAPFI9cG8j581NfXKxQKKZFILNqeSCTU2Nh43+0jkYgikUi+lwGHltqARAelhgYg8fcBHi7vv+0SDofV1dWlgYGB7LZMJqOBgQHFYrF83x08iAZAA5DoAA9XkJddenp69PLLL2vLli3aunWr3n//fd26dUuvvPJKIe4OHkQDoAFIdIAHK8jwsXv3bv3www965513FI/HtXnzZp06deq+Nx2hdNEAaAASHeDBCvaG03379mnfvn2F+vEoAjQAGoBEB7gfn+0CAACsYvgAAABWMXwAAACrGD4AAIBVDB8AAMAqhg8AAGAVwwcAALCK4QMAAFjl/FNtH9XkX/1ETzRXu16GryWb3H/gEx2457oDGnCPBpBrA0UzfGx+64+aSpe7XoavVZVNu14CHXiA6w5owD0aQK4N8LILAACwiuEDAABYVTQvu7RE/le3y8Kul+Fry4MzrpdABx7gugMacI8GkGsDRTN8/LJiVGlTNMstSWWBu66XQAce4LoDGnCPBpBrA7zsAgAArGL4AAAAVhXNeatQIKOMMq6X4WuhgPv9Twfuue6ABtyjAeTaQNGc+QjJuF6C73nhOfDCGvzO9XPg+v7h/jlwff/I/TkomuEDAACUBoYPAABgVdG852NT+EfXS4CktOP7pwNvcNkBDXgDDSCXBopm+KgNRRTkRI1TGWWUmE05XQMduOe6AxpwjwaQawNFM3wsU0ihALG5NGsCrpdABx7gugMacI8GkGsDPHsAAMCqojnzcTYlzbpehM+FJLU6LoYO3HPdAQ24RwPItYGiGT5G03WaMSHXy/C18mBarctuO10DHbjnugMacI8GkGsDRTN8TGUqdDvDpxi6tNy4/1RbOnDPdQc04B4NINcGimb4yJiAMoa3qLiU8cAbTunAPdcd0IB7NIBcGyia4eO/pn6q5N2I62X4WnXZtH5becHpGujAPdcd0IB7NIBcGyia4eN8okW37nCazaXK5Smpye0a6MA91x3QgHs0gFwbKJrh4/Z0WOk7Za6X4Wu3g+4/zIkO3HPdAQ24RwPItYGiGT5MJiBl3L/nwM8yHtj/dOCe6w5owD0aQK4NFM3wkb5TpsAtfrXKpXTQ/b806MA91x3QgHs0gFwbKJrh4xeHUwoOj7lehq/N/my19Bu3a6AD91x3QAPu0QBybaBoho/g5C3NXueTDF0K1a10vQQ68ADXHdCAezSAXBvgF6UBAIBVDB8AAMAqhg8AAGAVwwcAALCK4QMAAFjF8AEAAKxi+AAAAFYxfAAAAKsYPgAAgFUMHwAAwCqGDwAAYBXDBwAAsIrhAwAAWMXwAQAArGL4AAAAVjF8AAAAqxg+AACAVQwfAADAqiUNH4cOHVIgEFj01dHRkb1+enpae/fuVV1dnSorK7Vr1y4lEom8LxruvP+HpH7aGs9+beycWHQ9DfjDwg7mG9iyZUv2ejoofRwLkIsln/nYsGGDxsfHs19nzpzJXnfgwAGdPHlSJ06c0OnTpzU2NqadO3fmdcFwb/36ZTo3tErnhlbpy9N1i66jAf+Y72C+gc8//zx7HR34A8cCPK5lS/6GZcvU2Nh43/YbN27o448/1qeffqpnn31WkvTJJ5/oySef1NmzZ/X000/nvlp4QmiZtOonIUlSeUUgu50G/GW+g/kG6urm/vKhA//gWIDHteTh4+LFi2publZ5eblisZj6+vrU1tamoaEhpdNpdXd3Z2/b0dGhtrY2DQ4OPjS2VCqlVCqVvTw1NfUYDwM2/Wl4VtGuCUXKA/rVr+4l9LgNSHRQjOY7KAvPXR4dHdWGDRs4FvhIvo8FNOAfS3rZJRqN6siRIzp16pQOHz6s4eFhbdu2TclkUvF4XOFwWDU1NYu+p6GhQfF4/KE/s6+vT9XV1dmv1tbWx3ogsGPzr8P6pz9U68i/rNTvfl+la9dmJSmnBiQ6KDYLO/iHd1ZIknbs2MGxwEcKcSygAf9Y0pmPHTt2ZP+8ceNGRaNRtbe36/jx46qoqHisBRw8eFA9PT3Zy1NTUwTnYX/zt5Hsn598UvrZz0N65unr+uyzz1RbW/vYP5cOisvCDlpa5k6737hxg2OBjxTiWEAD/rHkl10Wqqmp0fr163Xp0iU999xzmpmZ0eTk5KJpN5FIPPA9IvMikYgikchDr4e3VVXNnTy7cuWKOjs7H6sBiQ5Kwbp16zgW+Fg+jgU04B85/T8fN2/e1OXLl9XU1KSuri6VlZVpYGAge/2FCxc0MjKiWCyW80LhTbdvZSRJjY2NNOBzw8PDHAt8jGMBlmJJZz7eeustPf/882pvb9fY2Jh6e3sVCoW0Z88eVVdX69VXX1VPT49qa2tVVVWl/fv3KxaL8c7mEvL7303pN93lamkJKpHI6J//MSlJevHFF2nARxZ2MDw891o/xwJ/4ViAXCxp+Lh69ar27NmjH3/8UatWrdIzzzyjs2fPatWqVZKk9957T8FgULt27VIqldL27dv10UcfFWThcCM+ntHf75vU5GRGtbVBbf51mSSpvr5eEg34xcIOalbOnUD94osvOBb4CMcC5GJJw8fRo0f/4vXl5eXq7+9Xf39/TouCd33wUc2iy8lkRqf+497/bEgD/rCwg2Qyo42dE1q7dm12Gx2UPo4FyAWf7QIAAKxi+AAAAFYxfAAAAKsYPgAAgFUMHwAAwCqGDwAAYBXDBwAAsIrhAwAAWMXwAQAArGL4AAAAVjF8AAAAqxg+AACAVQwfAADAKoYPAABgFcMHAACwiuEDAABYxfABAACsYvgAAABWMXwAAACrGD4AAIBVDB8AAMAqhg8AAGAVwwcAALCK4QMAAFjF8AEAAKxi+AAAAFYxfAAAAKsYPgAAgFUMHwAAwCqGDwAAYBXDBwAAsIrhAwAAWMXwAQAArFrmegEPczldpSfSoXsbjHG3GEdCdbUa3t/xSLdtP3lDwctXNTt5o2DrCaTv6t+Sv1y0bfrmXUkTBbtPOvB+BzRQeF5vQCpsBzRQeg14dvgYv1ujivS95QVmMw5X40ZgRaW6/u7bR7rt//z3L1QVf0IqYGzKZPTNjTWLNqVvzRTu/kQHkvc7oIHC83oDUmE7oIHSa8Bzw4f580R75+bsou13MyndNWkXS3Ink3rkJ/Nuerrw++gB65m/bPL8LxE6WMDjHdCABR5vQCpMBzSwQIk1EDD5PmLk6OrVq2ptbXW9DCzR6OioWlpa8vbzrly5onXr1uXt56Hw8t0Ax4LilM8OaKA4PUoDnhs+MpmMLly4oM7OTo2Ojqqqqsr1kqybmppSa2trUTx+Y4ySyaSam5sVDObv/cuTk5NauXKlRkZGVF1dnbefWyxogGOBRAc0ULoNeO5ll2AwqNWrV0uSqqqqPL+zC6lYHn8hhoP5cKurq4tiHxSK3xvgWDCnWB5/vjuggXuK5fE/agP8qi0AALCK4QMAAFjlyeEjEomot7dXkUjE9VKc8Pvjl9gHfn/88/y+H/z++CX2Qak+fs+94RQAAJQ2T575AAAApYvhAwAAWMXwAQAArGL4AAAAVnlu+Ojv79eaNWtUXl6uaDSqc+fOuV5SQRw6dEiBQGDRV0fHvU8snJ6e1t69e1VXV6fKykrt2rVLiUTC4YrtooM5fu6ABub4uQGJDuaVWgeeGj6OHTumnp4e9fb26vz589q0aZO2b9+uiYnCfVy3Sxs2bND4+Hj268yZM9nrDhw4oJMnT+rEiRM6ffq0xsbGtHPnToertYcO6IAGaECig5LuwHjI1q1bzd69e7OXZ2dnTXNzs+nr63O4qsLo7e01mzZteuB1k5OTpqyszJw4cSK77bvvvjOSzODgoKUVukMHc/zcAQ3M8XMDxtDBvFLswDNnPmZmZjQ0NKTu7u7stmAwqO7ubg0ODjpcWeFcvHhRzc3NWrt2rV566SWNjIxIkoaGhpROpxfti46ODrW1tZXsvphHB3RAAzQg0UGpd+CZ4eP69euanZ1VQ0PDou0NDQ2Kx+OOVlU40WhUR44c0alTp3T48GENDw9r27ZtSiaTisfjCofDqqmpWfQ9pbovFqIDOqABGpDooNQ78Nyn2vrFjh07sn/euHGjotGo2tvbdfz4cVVUVDhcGWyiA9AAJP914JkzH/X19QqFQve9ezeRSKixsdHRquypqanR+vXrdenSJTU2NmpmZkaTk5OLbuOHfUEHdEADNCDRQal34JnhIxwOq6urSwMDA9ltmUxGAwMDisViDldmx82bN3X58mU1NTWpq6tLZWVli/bFhQsXNDIyUvL7gg7ogAZoQKKDku/A9TteFzp69KiJRCLmyJEj5ttvvzWvvfaaqampMfF43PXS8u7NN980X331lRkeHjZff/216e7uNvX19WZiYsIYY8zrr79u2trazJdffmm++eYbE4vFTCwWc7xqO+iADmiABoyhg1LuwFPDhzHGfPDBB6atrc2Ew2GzdetWc/bsWddLKojdu3ebpqYmEw6HzerVq83u3bvNpUuXstffuXPHvPHGG2blypVm+fLl5oUXXjDj4+MOV2wXHczxcwc0MMfPDRhDB/NKrYOAMca4PvsCAAD8wzPv+QAAAP7A8AEAAKxi+AAAAFYxfAAAAKsYPgAAgFUMHwAAwCqGDwAAYBXDBwAAsIrhAwAAWMXwAQAArGL4AAAAVjF8AAAAq/4P2dkFdYATdQ4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_downsample = BatchCrop(size=(84,84))\n",
    "batch_downsample = batch_downsample(downsampled_stack)\n",
    "imglistshow(batch_downsample.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "084ae4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchToTensor:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, batch: np.array):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch (np.array): expects a numpy array\n",
    "        Returns:\n",
    "            torch.Tensor: torch tensor on device\n",
    "        \"\"\"\n",
    "        return torch.from_numpy(batch).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0eb71d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformPipeline:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __call__(self, batch: torch.Tensor):\n",
    "        for transform in self.transforms:\n",
    "            batch = transform(batch)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33e8702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = TransformPipeline([BatchToTensor(device), BatchGrayscale(), BatchDownsample(), BatchCrop()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c7b4df0",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1705481786623,
     "user": {
      "displayName": "Gareth Davies",
      "userId": "11833826550098318581"
     },
     "user_tz": 0
    },
    "id": "2c7b4df0"
   },
   "outputs": [],
   "source": [
    "def test_should_preprocess_to_correct_shape():   \n",
    "    pipeline = TransformPipeline([BatchToTensor(device), BatchGrayscale(), BatchDownsample(), BatchCrop()]) \n",
    "    env = TorchEnv(gym.make(environment))\n",
    "    s = env.reset()\n",
    "    preprocessed_s_prime = pipeline(s)\n",
    "    assert preprocessed_s_prime.shape == (4,1,84,84) # 4 frames, 1 channel, 84x84\n",
    "\n",
    "test_should_preprocess_to_correct_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083ac764",
   "metadata": {
    "id": "083ac764"
   },
   "source": [
    "## fill replay memory\n",
    "here we fill the replay memory with downsampled cropped stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9eac8602",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1705481796263,
     "user": {
      "displayName": "Gareth Davies",
      "userId": "11833826550098318581"
     },
     "user_tz": 0
    },
    "id": "9eac8602"
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, maxlen: int) -> None:\n",
    "        self.deque = deque(maxlen=maxlen)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def append(self, x: tuple)-> None:\n",
    "        self.deque.append(x)\n",
    "\n",
    "    def sample(self, bs: int) -> list:\n",
    "        return random.sample(self.deque, min(len(self), bs))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.deque)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "    def save(self, path: str):\n",
    "        with open(path, 'wb') as f:\n",
    "            return pickle.dump(self, f)\n",
    "\n",
    "def fill(replay_memory:ExperienceReplay, env:TorchEnv, fill_size:int = replay_start_size) -> None :\n",
    "    while len(replay_memory)<fill_size:\n",
    "        s = env.reset()\n",
    "        terminated=False\n",
    "        while terminated == False and len(replay_memory)<fill_size:\n",
    "            a = env.sample()\n",
    "            s_prime, r, terminated = env.step(a)\n",
    "            s_prime = torch.cat([s, s_prime])[-4:]\n",
    "            replay_memory.append((s, a, r, s_prime, terminated))\n",
    "            s = s_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d0c4934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 84, 84])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = TorchEnv(gym.make(environment), transforms=pipeline)\n",
    "replay_memory = ExperienceReplay(replay_memory_size)\n",
    "fill(replay_memory, env, fill_size=10)\n",
    "samples = replay_memory.sample(3)\n",
    "s, a, r, s_prime, terminated = samples[0]\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbcdfc51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1705481800746,
     "user": {
      "displayName": "Gareth Davies",
      "userId": "11833826550098318581"
     },
     "user_tz": 0
    },
    "id": "bbcdfc51",
    "outputId": "89ee91f7-27f6-49a5-9c7f-0108126bcb68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imglistshow(s.squeeze(1).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2554b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.data_types.image.Image at 0x1262ca4a0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.Image(s[:2].cpu(), caption=\"Top: Frame at t , Bottom: Frame at t+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "615773f0-eb08-4aa4-b86f-fbcb02171285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Efficient Replay Memory\n",
    "def low_mem_collate(batch):\n",
    "    s_j, a_j, r_j, s_prime_j, terminated_j = batch\n",
    "    return (s_j.squeeze(2).to(device), \n",
    "            a_j.to(device), \n",
    "            r_j.to(device), \n",
    "            s_prime_j.squeeze(2).to(device), \n",
    "            (~terminated_j).to(device).half()\n",
    "           )\n",
    "samples = replay_memory.sample(3)\n",
    "s_j, a_j, r_j, s_prime_j, terminated_j = low_mem_collate(samples)\n",
    "imglistshow(s_j[0].cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee5f456",
   "metadata": {
    "id": "bee5f456"
   },
   "source": [
    "## Define the Atari DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e03eb896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNDeepmind2013(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super(DQNDeepmind2013, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, 8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*9*9, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, s):\n",
    "        return self.conv(s) \n",
    "    \n",
    "class DQNDeepmind2015(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super(DQNDeepmind2015, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, s):\n",
    "        return self.conv(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad53db6c",
   "metadata": {
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1705481813206,
     "user": {
      "displayName": "Gareth Davies",
      "userId": "11833826550098318581"
     },
     "user_tz": 0
    },
    "id": "ad53db6c"
   },
   "outputs": [],
   "source": [
    "class AtariDQN(nn.Module):\n",
    "    def __init__(self, net, n_actions, normalize=True):\n",
    "        super(AtariDQN, self).__init__()\n",
    "        self.net = net\n",
    "        self.n_actions = n_actions\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, s: torch.Tensor):\n",
    "        '''\n",
    "        Args:\n",
    "            s (torch.Tensor): batch of shape (B, H, W)\n",
    "        Returns:\n",
    "            torch.Tensor: batch of shape (B, n_actions)\n",
    "        '''\n",
    "        if self.normalize:\n",
    "            s = s.float() / 255.0\n",
    "        return self.net(s)\n",
    "\n",
    "    def select_next_action(self, s, epsilon):\n",
    "        with torch.no_grad():\n",
    "            use_greedy = np.random.binomial(1, 1-epsilon)\n",
    "            if use_greedy:\n",
    "                a = self(s).argmax().item()\n",
    "            else:\n",
    "                a = np.random.randint(self.n_actions)\n",
    "            return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829732d1",
   "metadata": {
    "id": "829732d1"
   },
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7298055d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T09:57:38.319912Z",
     "iopub.status.busy": "2024-01-20T09:57:38.319145Z",
     "iopub.status.idle": "2024-01-20T09:57:38.343313Z",
     "shell.execute_reply": "2024-01-20T09:57:38.342791Z",
     "shell.execute_reply.started": "2024-01-20T09:57:38.319886Z"
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1705481815123,
     "user": {
      "displayName": "Gareth Davies",
      "userId": "11833826550098318581"
     },
     "user_tz": 0
    },
    "id": "7298055d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = AtariDQN(DQNDeepmind2015(env.n_actions).to(device), n_actions=env.n_actions).to(device)\n",
    "if BASE_MODEL is not None:\n",
    "    print(f\"Loading base model from {BASE_MODEL}\")\n",
    "    dqn.load_state_dict(torch.load(BASE_MODEL))\n",
    "\n",
    "target_net = AtariDQN(DQNDeepmind2015(env.n_actions).to(device), n_actions=env.n_actions).to(device)\n",
    "target_net.load_state_dict(dqn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b72ac1cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-01-20T09:57:44.752327Z",
     "iopub.status.busy": "2024-01-20T09:57:44.751896Z",
     "iopub.status.idle": "2024-01-20T09:57:44.853896Z",
     "shell.execute_reply": "2024-01-20T09:57:44.853289Z",
     "shell.execute_reply.started": "2024-01-20T09:57:44.752298Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1705481817498,
     "user": {
      "displayName": "Gareth Davies",
      "userId": "11833826550098318581"
     },
     "user_tz": 0
    },
    "id": "b72ac1cb",
    "outputId": "fead66f6-f24a-46bd-dffd-bf4d7238f945"
   },
   "outputs": [],
   "source": [
    "def test_should_return_value_for_each_action():\n",
    "    n_actions = 4\n",
    "    dqn = AtariDQN(DQNDeepmind2015(n_actions), n_actions=n_actions)\n",
    "    samples = torch.randn(32, 4, 1, 84, 84)\n",
    "    out = dqn(samples[0].squeeze(1).unsqueeze(0)).squeeze(0)\n",
    "    assert out.shape[0] == env.n_actions\n",
    "\n",
    "test_should_return_value_for_each_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6813a739",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-01-20T09:58:06.802828Z",
     "iopub.status.busy": "2024-01-20T09:58:06.802260Z",
     "iopub.status.idle": "2024-01-20T09:58:06.807862Z",
     "shell.execute_reply": "2024-01-20T09:58:06.807292Z",
     "shell.execute_reply.started": "2024-01-20T09:58:06.802804Z"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1705481838021,
     "user": {
      "displayName": "Gareth Davies",
      "userId": "11833826550098318581"
     },
     "user_tz": 0
    },
    "id": "6813a739",
    "outputId": "6bfe645a-b84d-491e-f374-b356f68b6006"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtariDQN(\n",
       "  (net): DQNDeepmind2015(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "      (7): Linear(in_features=3136, out_features=512, bias=True)\n",
       "      (8): ReLU()\n",
       "      (9): Linear(in_features=512, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = Optimizer(dqn.parameters(),  lr=lr, eps=adam_epsilon)\n",
    "dqn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85da894e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T09:58:31.955949Z",
     "iopub.status.busy": "2024-01-20T09:58:31.955285Z",
     "iopub.status.idle": "2024-01-20T09:58:31.960596Z",
     "shell.execute_reply": "2024-01-20T09:58:31.960147Z",
     "shell.execute_reply.started": "2024-01-20T09:58:31.955918Z"
    },
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1705481860023,
     "user": {
      "displayName": "Gareth Davies",
      "userId": "11833826550098318581"
     },
     "user_tz": 0
    },
    "id": "85da894e"
   },
   "outputs": [],
   "source": [
    "def atari_collate(batch):\n",
    "    s_j, a_j, r_j, s_prime_j, terminated_j = list(zip(*batch))\n",
    "    return (torch.stack(s_j).squeeze(2).to(device), \n",
    "            torch.tensor(a_j).to(device), \n",
    "            torch.tensor(r_j).to(device), \n",
    "            torch.stack(s_prime_j).squeeze(2).to(device), \n",
    "            (~torch.tensor(terminated_j)).to(device).half()\n",
    "    )\n",
    "def get_batch_efficient(self, batch, target_net=None, collate_fn=None):\n",
    "    if target_net is None:\n",
    "        target_net = self\n",
    "    s, a, r, s_prime, not_terminated = collate_fn(batch)\n",
    "\n",
    "    y_hat = self(s).gather(1, a.unsqueeze(1)).squeeze() # gather the values at the indices given by the actions a\n",
    "\n",
    "    next_values, _ = target_net(s_prime).max(dim=1)\n",
    "    next_values = next_values.clone().detach()\n",
    "    y_j = r.detach().clone() + gamma * next_values * not_terminated # if terminated then not_terminated is set to zero (y_j = r)\n",
    "    return y_hat, y_j\n",
    "\n",
    "def get_epsilon(epsilon, final_epsilon=0.1, steps_to_anneal=final_exploration_frame):\n",
    "    if epsilon > final_epsilon:\n",
    "        epsilon -= (1/steps_to_anneal) * (1-final_epsilon)\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46a0f0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05421686746987952"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate(dqn, env):\n",
    "    k = 0\n",
    "    total_rewards = []\n",
    "    while k < 15*60*5: #15 frames / sec * 60 seconds * 5 minutes\n",
    "        terminated = False\n",
    "        rewards = 0\n",
    "        s = env.reset()\n",
    "        while terminated == False:\n",
    "            a = dqn.select_next_action(s.squeeze(1).unsqueeze(0), 0.1)\n",
    "            s_prime, r, terminated = env.step(a)\n",
    "            s_prime = torch.cat([s, s_prime])[-4:]\n",
    "            s = s_prime\n",
    "            rewards += r\n",
    "            k += 1\n",
    "        total_rewards.append(rewards)\n",
    "    return sum(total_rewards)/len(total_rewards)\n",
    "\n",
    "validate(dqn, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "0c367002",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-01-20T09:58:34.394285Z",
     "iopub.status.busy": "2024-01-20T09:58:34.393473Z",
     "iopub.status.idle": "2024-01-20T09:58:34.398973Z",
     "shell.execute_reply": "2024-01-20T09:58:34.398239Z",
     "shell.execute_reply.started": "2024-01-20T09:58:34.394248Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1705481873718,
     "user": {
      "displayName": "Gareth Davies",
      "userId": "11833826550098318581"
     },
     "user_tz": 0
    },
    "id": "0c367002",
    "outputId": "39c78315-d9ae-4f54-fc36-1cb33346a2e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = FRAMES_TO_TRAIN // EPOCH_SIZE\n",
    "print(n_epochs)\n",
    "track = True\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss, epoch_reward, k, epoch_episodes = 0, 0, 0, 0\n",
    "    while k < EPOCH_SIZE:\n",
    "        terminated = False\n",
    "        s = env.reset()\n",
    "        while terminated == False:\n",
    "            a = dqn.select_next_action(s.squeeze(1).unsqueeze(0), epsilon)\n",
    "            s_prime, r, terminated = env.step(a)\n",
    "            s_prime = torch.cat([s, s_prime])[-4:]\n",
    "            replay_memory.append((s, a, r, s_prime, terminated))\n",
    "            epsilon = get_epsilon(epsilon)\n",
    "            s = s_prime\n",
    "\n",
    "            if len(replay_memory) > replay_start_size and k % replay_period == 0:\n",
    "                optimizer.zero_grad()\n",
    "                batch = replay_memory.sample(bs)\n",
    "                y_hat, y = get_batch_efficient(dqn, batch, target_net=target_net, collate_fn=atari_collate)\n",
    "                loss = loss_fn(y_hat, y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_value_(dqn.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                        \n",
    "                if k % sync_every_n_steps == 0:\n",
    "                    target_net.load_state_dict(dqn.state_dict())\n",
    "            \n",
    "                loss = loss.detach()\n",
    "                epoch_loss += loss.item()\n",
    "            epoch_reward += r\n",
    "            k += 1\n",
    "        epoch_episodes += 1\n",
    "    \n",
    "    if track:\n",
    "        validate_reward = validate(dqn, env)\n",
    "        torch.save(dqn.state_dict(), 'breakout.pt')\n",
    "        wandb.log_model(name=f\"breakout-{wandb.run.id}\", path='breakout.pt')\n",
    "        wandb.log({\"epoch\":epoch,\n",
    "                \"step_loss\": epoch_loss / k, \n",
    "                \"reward\": epoch_reward / epoch_episodes,\n",
    "                \"step\":k,\n",
    "                \"epsilon\": epsilon,\n",
    "                \"validate_reward\":validate_reward\n",
    "                })\n",
    "        print(f'epochs {epoch}, step_loss {epoch_loss / k}, reward {epoch_reward / epoch_episodes}, k {k}, epsilon {epsilon}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "587a8390-ea72-41f5-a771-9a9b3e9524d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dump() missing required argument 'file' (pos 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/garethdavies/Development/workspaces/rl/dqn/dqn-atari.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/garethdavies/Development/workspaces/rl/dqn/dqn-atari.ipynb#Y404sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m replay_memory\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39mreplay_memory.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/garethdavies/Development/workspaces/rl/dqn/dqn-atari.ipynb Cell 40\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/garethdavies/Development/workspaces/rl/dqn/dqn-atari.ipynb#Y404sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/garethdavies/Development/workspaces/rl/dqn/dqn-atari.ipynb#Y404sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave\u001b[39m(\u001b[39mcls\u001b[39m, path: \u001b[39mstr\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/garethdavies/Development/workspaces/rl/dqn/dqn-atari.ipynb#Y404sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/garethdavies/Development/workspaces/rl/dqn/dqn-atari.ipynb#Y404sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m pickle\u001b[39m.\u001b[39;49mdump(f)\n",
      "\u001b[0;31mTypeError\u001b[0m: dump() missing required argument 'file' (pos 2)"
     ]
    }
   ],
   "source": [
    "replay_memory.save('replay_memory.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "58c7ea61-6e62-4d27-b7ba-22b0546da653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = TorchEnv(gym.make(environment), transforms=pipeline, terminate_after_lose_life=False)\n",
    "dqn = AtariDQN(DQNDeepmind2015(env.n_actions).to(device), n_actions=env.n_actions).to(device)\n",
    "dqn.load_state_dict(torch.load('breakout (26).pt', map_location=torch.device('cpu')))\n",
    "\n",
    "def validate(dqn, env):\n",
    "    k = 0\n",
    "    total_rewards = []\n",
    "    while k < 15*60*5: #15 frames / sec * 60 seconds * 5 minutes\n",
    "        terminated = False\n",
    "        rewards = 0\n",
    "        s = env.reset()\n",
    "        while terminated == False:\n",
    "            a = dqn.select_next_action(s.squeeze(1).unsqueeze(0), 0.1)\n",
    "            s_prime, r, terminated = env.step(a)\n",
    "            s_prime = torch.cat([s, s_prime])[-4:]\n",
    "            s = s_prime\n",
    "            rewards += r\n",
    "            k += 1\n",
    "        total_rewards.append(rewards)\n",
    "    return sum(total_rewards)/len(total_rewards)\n",
    "\n",
    "validate(dqn, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "2e7b660f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.009 MB uploaded\\r'), FloatProgress(value=0.10711586221790304, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>epsilon</td><td>█▆▃▁</td></tr><tr><td>reward</td><td>▁█▃█</td></tr><tr><td>step</td><td>█▄▁█</td></tr><tr><td>step_loss</td><td>█▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>epsilon</td><td>0.99636</td></tr><tr><td>reward</td><td>1.5283</td></tr><tr><td>step</td><td>10160</td></tr><tr><td>step_loss</td><td>0.00035</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comfy-armadillo-5</strong> at: <a href='https://wandb.ai/garethmd/atari/runs/1zvxlry0' target=\"_blank\">https://wandb.ai/garethmd/atari/runs/1zvxlry0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240119_171245-1zvxlry0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "96409362-4fbc-48fc-9a16-07322ea6eb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "WRITERS[\"opencv\"](iter(env.images), \"breakout-26.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bafc4752-78e6-4924-b6d6-db49eebce7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, Iterator, Literal, Optional\n",
    "\n",
    "import cv2\n",
    "import ffmpeg\n",
    "import matplotlib.animation as ani\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def as_uint8(arr: np.ndarray) -> np.ndarray:\n",
    "    if np.issubdtype(arr.dtype, np.integer):\n",
    "        return arr.astype(np.uint8)\n",
    "    if np.issubdtype(arr.dtype, np.floating):\n",
    "        return (arr * 255).round().astype(np.uint8)\n",
    "    raise NotImplementedError(f\"Unsupported dtype: {arr.dtype}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VideoProps:\n",
    "    frame_width: int\n",
    "    frame_height: int\n",
    "    frame_count: int\n",
    "    fps: int\n",
    "\n",
    "    @classmethod\n",
    "    def from_file_opencv(cls, fpath: str) -> \"VideoProps\":\n",
    "        cap = cv2.VideoCapture(str(fpath))\n",
    "\n",
    "        return cls(\n",
    "            frame_width=int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "            frame_height=int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
    "            frame_count=int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "            fps=int(cap.get(cv2.CAP_PROP_FPS)),\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_file_ffmpeg(cls, fpath: str) -> \"VideoProps\":\n",
    "        probe = ffmpeg.probe(str(fpath))\n",
    "\n",
    "        for stream in probe[\"streams\"]:\n",
    "            if stream[\"codec_type\"] == \"video\":\n",
    "                width, height, count = stream[\"width\"], stream[\"height\"], int(stream[\"nb_frames\"])\n",
    "                fps_num, fps_denom = stream[\"r_frame_rate\"].split(\"/\")\n",
    "                assert fps_denom == \"1\", f\"Unexpected frame rate: {stream['r_frame_rate']}\"\n",
    "                fps = int(fps_num)\n",
    "                return cls(\n",
    "                    frame_width=width,\n",
    "                    frame_height=height,\n",
    "                    frame_count=count,\n",
    "                    fps=fps,\n",
    "                )\n",
    "\n",
    "        raise ValueError(f\"Could not parse video properties from video in {fpath}\")\n",
    "\n",
    "\n",
    "def read_video_ffmpeg(\n",
    "    in_file: str,\n",
    "    output_fmt: str = \"rgb24\",\n",
    "    channels: int = 3,\n",
    ") -> Iterator[np.ndarray]:\n",
    "    \"\"\"Function that reads a video to a stream of numpy arrays using FFMPEG.\n",
    "\n",
    "    Args:\n",
    "        in_file: The input video to read\n",
    "        output_fmt: The output image format\n",
    "        channels: Number of output channels for each video frame\n",
    "\n",
    "    Yields:\n",
    "        Frames from the video as numpy arrays with shape (H, W, C)\n",
    "    \"\"\"\n",
    "\n",
    "    props = VideoProps.from_file_ffmpeg(in_file)\n",
    "\n",
    "    stream = ffmpeg.input(str(in_file))\n",
    "    stream = ffmpeg.output(stream, \"pipe:\", format=\"rawvideo\", pix_fmt=output_fmt, r=props.fps)\n",
    "    stream = ffmpeg.run_async(stream, pipe_stdout=True)\n",
    "\n",
    "    while True:\n",
    "        in_bytes = stream.stdout.read(props.frame_width * props.frame_height * channels)\n",
    "        if not in_bytes:\n",
    "            break\n",
    "        yield np.frombuffer(in_bytes, np.uint8).reshape((props.frame_height, props.frame_width, channels))\n",
    "\n",
    "    stream.stdout.close()\n",
    "    stream.wait()\n",
    "\n",
    "\n",
    "def read_video_opencv(in_file: str) -> Iterator[np.ndarray]:\n",
    "    \"\"\"Reads a video as a stream using OpenCV.\n",
    "\n",
    "    Args:\n",
    "        in_file: The input video to read\n",
    "\n",
    "    Yields:\n",
    "        Frames from the video as numpy arrays with shape (H, W, C)\n",
    "    \"\"\"\n",
    "\n",
    "    cap = cv2.VideoCapture(str(in_file))\n",
    "\n",
    "    while True:\n",
    "        ret, buffer = cap.read()\n",
    "        if not ret:\n",
    "            cap.release()\n",
    "            return\n",
    "        yield buffer\n",
    "\n",
    "\n",
    "def write_video_opencv(\n",
    "    itr: Iterator[np.ndarray],\n",
    "    out_file: str,\n",
    "    fps: int = 30,\n",
    "    codec: str = \"MP4V\",\n",
    ") -> None:\n",
    "    \"\"\"Function that writes a video from a stream of numpy arrays using OpenCV.\n",
    "\n",
    "    Args:\n",
    "        itr: The image iterator, yielding images with shape (H, W, C).\n",
    "        out_file: The path to the output file.\n",
    "        fps: Frames per second for the video.\n",
    "        codec: FourCC code specifying OpenCV video codec type. Examples are\n",
    "            MPEG, MP4V, DIVX, AVC1, H236.\n",
    "    \"\"\"\n",
    "\n",
    "    first_img = next(itr)\n",
    "    height, width, _ = first_img.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "    stream = cv2.VideoWriter(str(out_file), fourcc, fps, (width, height))\n",
    "\n",
    "    def write_frame(img: np.ndarray) -> None:\n",
    "        stream.write(as_uint8(img))\n",
    "\n",
    "    write_frame(first_img)\n",
    "    for img in itr:\n",
    "        write_frame(img)\n",
    "\n",
    "    stream.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def write_video_ffmpeg(\n",
    "    itr: Iterator[np.ndarray],\n",
    "    out_file: str,\n",
    "    fps: int = 30,\n",
    "    out_fps: int = 30,\n",
    "    vcodec: str = \"libx264\",\n",
    "    input_fmt: str = \"rgb24\",\n",
    "    output_fmt: str = \"yuv420p\",\n",
    ") -> None:\n",
    "    \"\"\"Function that writes an video from a stream of numpy arrays using FFMPEG.\n",
    "\n",
    "    Args:\n",
    "        itr: The image iterator, yielding images with shape (H, W, C).\n",
    "        out_file: The path to the output file.\n",
    "        fps: Frames per second for the video.\n",
    "        out_fps: Frames per second for the saved video.\n",
    "        vcodec: The video codec to use for the output video\n",
    "        input_fmt: The input image format\n",
    "        output_fmt: The output image format\n",
    "    \"\"\"\n",
    "\n",
    "    first_img = next(itr)\n",
    "    height, width, _ = first_img.shape\n",
    "\n",
    "    stream = ffmpeg.input(\"pipe:\", format=\"rawvideo\", pix_fmt=input_fmt, s=f\"{width}x{height}\", r=fps)\n",
    "    stream = ffmpeg.output(stream, str(out_file), pix_fmt=output_fmt, vcodec=vcodec, r=out_fps)\n",
    "    stream = ffmpeg.overwrite_output(stream)\n",
    "    stream = ffmpeg.run_async(stream, pipe_stdin=True)\n",
    "\n",
    "    def write_frame(img: np.ndarray) -> None:\n",
    "        stream.stdin.write(as_uint8(img).tobytes())\n",
    "\n",
    "    # Writes all the video frames to the file.\n",
    "    write_frame(first_img)\n",
    "    for img in itr:\n",
    "        write_frame(img)\n",
    "\n",
    "    stream.stdin.close()\n",
    "    stream.wait()\n",
    "\n",
    "\n",
    "def write_video_matplotlib(\n",
    "    itr: Iterator[np.ndarray],\n",
    "    out_file: str,\n",
    "    dpi: int = 50,\n",
    "    fps: int = 30,\n",
    "    title: str = \"Video\",\n",
    "    comment: Optional[str] = None,\n",
    "    writer: str = \"ffmpeg\",\n",
    ") -> None:\n",
    "    \"\"\"Function that writes an video from a stream of input tensors.\n",
    "\n",
    "    Args:\n",
    "        itr: The image iterator, yielding images with shape (H, W, C).\n",
    "        out_file: The path to the output file.\n",
    "        dpi: Dots per inch for output image.\n",
    "        fps: Frames per second for the video.\n",
    "        title: Title for the video metadata.\n",
    "        comment: Comment for the video metadata.\n",
    "        writer: The Matplotlib video writer to use (if you use the\n",
    "            default one, make sure you have `ffmpeg` installed on your\n",
    "            system).\n",
    "    \"\"\"\n",
    "\n",
    "    first_img = next(itr)\n",
    "    height, width, _ = first_img.shape\n",
    "    fig, ax = plt.subplots(figsize=(width / dpi, height / dpi))\n",
    "\n",
    "    # Ensures that there's no extra space around the image.\n",
    "    fig.subplots_adjust(\n",
    "        left=0,\n",
    "        bottom=0,\n",
    "        right=1,\n",
    "        top=1,\n",
    "        wspace=None,\n",
    "        hspace=None,\n",
    "    )\n",
    "\n",
    "    # Creates the writer with the given metadata.\n",
    "    writer_obj = ani.writers[writer]\n",
    "    metadata = {\n",
    "        \"title\": title,\n",
    "        \"artist\": __name__,\n",
    "        \"comment\": comment,\n",
    "    }\n",
    "    mpl_writer = writer_obj(\n",
    "        fps=fps,\n",
    "        metadata={k: v for k, v in metadata.items() if v is not None},\n",
    "    )\n",
    "\n",
    "    with mpl_writer.saving(fig, out_file, dpi=dpi):\n",
    "        im = ax.imshow(as_uint8(first_img), interpolation=\"nearest\")\n",
    "        mpl_writer.grab_frame()\n",
    "\n",
    "        for img in itr:\n",
    "            im.set_data(as_uint8(img))\n",
    "            mpl_writer.grab_frame()\n",
    "\n",
    "\n",
    "Reader = Literal[\"ffmpeg\", \"opencv\"]\n",
    "Writer = Literal[\"ffmpeg\", \"matplotlib\", \"opencv\"]\n",
    "\n",
    "READERS: Dict[Reader, Callable[[str], Iterator[np.ndarray]]] = {\n",
    "    \"ffmpeg\": read_video_ffmpeg,\n",
    "    \"opencv\": read_video_opencv,\n",
    "}\n",
    "\n",
    "WRITERS: Dict[Writer, Callable[[Iterator[np.ndarray], str], None]] = {\n",
    "    \"ffmpeg\": write_video_ffmpeg,\n",
    "    \"matplotlib\": write_video_matplotlib,\n",
    "    \"opencv\": write_video_opencv,\n",
    "}\n",
    "\n",
    "# Remove the FFMPEG reader and writer if FFMPEG is not available in the system.\n",
    "if not shutil.which(\"ffmpeg\"):\n",
    "    READERS.pop(\"ffmpeg\")\n",
    "    WRITERS.pop(\"ffmpeg\")\n",
    "    WRITERS.pop(\"matplotlib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "95b6fcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1331, 3.1186, 3.1245, 3.1267]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1724, 3.1563, 3.1611, 3.1671]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1482, 3.1338, 3.1381, 3.1462]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1731, 3.1577, 3.1627, 3.1705]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1512, 3.1224, 3.1316, 3.1465]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0785, 3.0421, 3.0541, 3.0791]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2077, 3.1701, 3.1721, 3.2142]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3163, 3.2706, 3.2784, 3.3198]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3963, 3.3538, 3.3632, 3.3897]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3996, 3.3793, 3.3913, 3.3810]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2257, 3.2033, 3.2087, 3.2306]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2619, 3.2486, 3.2538, 3.2515]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2966, 3.2778, 3.2878, 3.2812]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4176, 3.3954, 3.4062, 3.3914]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2865, 3.2565, 3.3079, 3.2648]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2691, 3.1057, 3.1680, 3.4259]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3632, 3.3417, 3.3470, 3.3574]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5937, 3.5690, 3.5749, 3.5815]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6430, 3.6125, 3.6316, 3.5947]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6828, 3.6532, 3.6666, 3.6379]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6698, 3.6398, 3.6647, 3.6246]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7059, 3.6787, 3.7043, 3.6689]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6446, 3.6332, 3.6558, 3.6289]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5559, 3.5410, 3.5738, 3.5263]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6105, 3.6023, 3.6180, 3.5819]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5544, 3.5451, 3.5536, 3.5541]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5526, 3.5507, 3.5630, 3.5567]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5157, 3.5157, 3.5353, 3.5243]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4962, 3.4781, 3.4906, 3.5012]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6306, 3.6206, 3.6226, 3.6458]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6931, 3.6850, 3.6961, 3.6984]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6339, 3.6249, 3.6412, 3.6329]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7775, 3.7676, 3.7797, 3.7699]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8147, 3.7967, 3.8039, 3.8224]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7825, 3.7629, 3.7736, 3.7784]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.9079, 3.8863, 3.8927, 3.9073]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.9331, 3.9165, 3.9269, 3.9370]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.9157, 3.9049, 3.9201, 3.9175]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.9061, 3.8899, 3.8951, 3.8948]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8329, 3.7695, 3.8322, 3.7758]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7378, 3.7120, 3.7145, 3.7250]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7471, 3.7227, 3.7268, 3.7369]], grad_fn=<AddmmBackward0>)\n",
      "tensor(1.)\n",
      "tensor([[3.0345, 3.0253, 3.0239, 3.0247]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8665, 2.8701, 2.8738, 2.8547]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9243, 2.9177, 2.9252, 2.9275]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8816, 2.8709, 2.8833, 2.8778]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9024, 2.8915, 2.8998, 2.9014]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0026, 2.9964, 3.0064, 2.9947]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9747, 2.9668, 2.9742, 2.9741]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0216, 3.0139, 3.0190, 3.0253]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0075, 2.9976, 3.0073, 3.0025]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9805, 2.9672, 2.9784, 2.9862]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0498, 3.0393, 3.0483, 3.0482]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9824, 2.9751, 2.9798, 2.9898]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1331, 3.1161, 3.1227, 3.1451]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0950, 3.0732, 3.0796, 3.0957]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3216, 3.3093, 3.3131, 3.3151]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2528, 3.2360, 3.2379, 3.2503]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2305, 3.2121, 3.2181, 3.2188]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2319, 3.2116, 3.2215, 3.2147]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3430, 3.3240, 3.3307, 3.3306]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3684, 3.3418, 3.3514, 3.3483]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3966, 3.3797, 3.3867, 3.3778]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3600, 3.3425, 3.3508, 3.3396]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2869, 3.2734, 3.2849, 3.2576]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3147, 3.2870, 3.3023, 3.2799]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3463, 3.3082, 3.3253, 3.3058]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4233, 3.4047, 3.4152, 3.3938]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3798, 3.3520, 3.3685, 3.3437]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3056, 3.2882, 3.3045, 3.2751]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5248, 3.4827, 3.5060, 3.4772]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4190, 3.3486, 3.3831, 3.3740]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4285, 3.3928, 3.4169, 3.3825]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4894, 3.4063, 3.4529, 3.4225]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4890, 3.4574, 3.4748, 3.4493]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6566, 3.6092, 3.6316, 3.6075]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6717, 3.6112, 3.6395, 3.6261]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8050, 3.7832, 3.7845, 3.7767]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7237, 3.7271, 3.7526, 3.6979]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3284, 3.3480, 3.4013, 3.3102]], grad_fn=<AddmmBackward0>)\n",
      "tensor(1.)\n",
      "tensor([[2.7601, 2.7463, 2.7530, 2.7397]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7495, 2.7321, 2.7415, 2.7195]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8869, 2.8687, 2.8753, 2.8581]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8117, 2.7954, 2.8028, 2.7812]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7081, 2.6918, 2.7024, 2.6847]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7954, 2.7787, 2.7868, 2.7666]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7706, 2.7511, 2.7587, 2.7488]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7785, 2.7612, 2.7728, 2.7622]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7253, 2.7104, 2.7197, 2.7067]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8049, 2.7883, 2.7958, 2.7796]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9499, 2.9249, 2.9247, 2.9129]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8665, 2.8435, 2.8451, 2.8333]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8660, 2.8491, 2.8516, 2.8482]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8973, 2.8768, 2.8751, 2.8713]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9441, 2.9217, 2.9158, 2.9154]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0044, 2.9792, 2.9699, 2.9737]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9307, 2.9117, 2.9097, 2.9060]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9348, 2.9145, 2.9096, 2.9102]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9573, 2.9370, 2.9331, 2.9321]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9704, 2.9498, 2.9450, 2.9444]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9688, 2.9484, 2.9441, 2.9433]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9688, 2.9484, 2.9441, 2.9433]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9688, 2.9484, 2.9441, 2.9433]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9688, 2.9484, 2.9441, 2.9433]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9688, 2.9484, 2.9441, 2.9433]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9688, 2.9484, 2.9441, 2.9433]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9688, 2.9484, 2.9441, 2.9433]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9688, 2.9484, 2.9441, 2.9433]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9688, 2.9484, 2.9441, 2.9433]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9688, 2.9484, 2.9441, 2.9433]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0523, 3.0312, 3.0427, 3.0329]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1027, 3.0728, 3.0961, 3.0738]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0881, 3.0662, 3.0768, 3.0731]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1304, 3.1084, 3.1195, 3.1152]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0622, 3.0271, 3.0423, 3.0486]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9826, 2.9412, 2.9578, 2.9696]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0519, 2.9973, 3.0127, 3.0476]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0736, 3.0119, 3.0287, 3.0686]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0884, 3.0294, 3.0489, 3.0976]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1113, 3.0603, 3.0812, 3.1002]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1084, 3.0713, 3.0898, 3.0998]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9857, 2.9621, 2.9717, 2.9742]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9272, 2.9056, 2.9178, 2.9261]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9571, 2.9386, 2.9479, 2.9596]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2007, 3.1807, 3.1884, 3.1906]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0817, 3.0188, 3.0455, 3.0439]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4020, 3.3718, 3.3835, 3.3714]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3305, 3.2419, 3.2977, 3.2462]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4583, 3.4005, 3.4337, 3.3933]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5288, 3.5072, 3.5133, 3.4983]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6232, 3.5614, 3.6195, 3.5432]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5546, 3.5011, 3.5210, 3.5034]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4584, 3.3733, 3.4167, 3.3778]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3805, 3.3328, 3.3573, 3.3450]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5378, 3.5207, 3.5457, 3.4844]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4063, 3.3901, 3.4075, 3.3724]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5820, 3.5639, 3.5706, 3.5544]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6511, 3.6210, 3.6355, 3.6039]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5673, 3.5483, 3.5654, 3.5193]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5194, 3.5085, 3.5211, 3.4875]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5236, 3.5135, 3.5275, 3.4844]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5714, 3.5489, 3.5552, 3.5463]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6727, 3.6579, 3.6709, 3.6502]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4855, 3.4573, 3.4722, 3.4447]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5132, 3.4986, 3.5040, 3.4926]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5127, 3.4837, 3.4872, 3.4900]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4817, 3.4445, 3.4460, 3.4619]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3819, 3.3035, 3.3219, 3.3619]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6261, 3.4970, 3.5366, 3.6367]], grad_fn=<AddmmBackward0>)\n",
      "tensor(1.)\n",
      "tensor([[2.6963, 2.6751, 2.6829, 2.7204]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.6028, 2.5588, 2.5603, 2.6063]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9547, 2.8678, 2.9650, 2.9597]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.6563, 2.6311, 2.6568, 2.6535]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8197, 2.7608, 2.8216, 2.8129]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.6519, 2.6439, 2.6517, 2.6612]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.6608, 2.6528, 2.6605, 2.6702]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8820, 2.8740, 2.8812, 2.8828]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8075, 2.8015, 2.8121, 2.8023]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8397, 2.8131, 2.8382, 2.8466]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7705, 2.7570, 2.7716, 2.7786]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8316, 2.8174, 2.8260, 2.8293]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8619, 2.8545, 2.8613, 2.8727]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9375, 2.9348, 2.9375, 2.9532]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8532, 2.8538, 2.8533, 2.8716]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8164, 2.8073, 2.8109, 2.8181]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8869, 2.8754, 2.8785, 2.8791]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8842, 2.8706, 2.8754, 2.8710]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9764, 2.9614, 2.9680, 2.9704]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0044, 2.9579, 2.9998, 2.9986]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9267, 2.9114, 2.9190, 2.9236]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0110, 2.9944, 3.0015, 3.0064]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1476, 3.1290, 3.1346, 3.1401]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2195, 3.2017, 3.2063, 3.2089]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4929, 3.4585, 3.4676, 3.4514]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3814, 3.3563, 3.3741, 3.3374]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3303, 3.3070, 3.3269, 3.2917]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4021, 3.3860, 3.3950, 3.3812]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4436, 3.4166, 3.4250, 3.4119]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4489, 3.4204, 3.4251, 3.4261]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4829, 3.4618, 3.4665, 3.4731]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5222, 3.5099, 3.5183, 3.5142]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4299, 3.4147, 3.4195, 3.4174]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4278, 3.4059, 3.4149, 3.4069]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4791, 3.4633, 3.4728, 3.4608]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5993, 3.5905, 3.6058, 3.5782]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4930, 3.4854, 3.5051, 3.4724]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5347, 3.5314, 3.5490, 3.5216]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6744, 3.6685, 3.6850, 3.6670]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6988, 3.6894, 3.7032, 3.6912]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6496, 3.6333, 3.6452, 3.6579]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6684, 3.6522, 3.6623, 3.6683]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5585, 3.5250, 3.5589, 3.5281]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3982, 3.3234, 3.4006, 3.3411]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4773, 3.4567, 3.4584, 3.4726]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7690, 3.7411, 3.7442, 3.7486]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3176, 3.3042, 3.3062, 3.3152]], grad_fn=<AddmmBackward0>)\n",
      "tensor(1.)\n",
      "tensor([[2.7030, 2.6988, 2.7070, 2.7106]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.5926, 2.5949, 2.6054, 2.5873]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3739, 3.2651, 3.3721, 3.3640]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0871, 3.0256, 3.0751, 3.0862]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9980, 2.9684, 2.9726, 2.9930]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0524, 3.0340, 3.0399, 3.0469]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9341, 2.9201, 2.9257, 2.9355]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2165, 3.1730, 3.2033, 3.2093]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1131, 3.0871, 3.0903, 3.1112]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0650, 3.0184, 3.0487, 3.0628]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9620, 2.9442, 2.9501, 2.9599]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0335, 3.0144, 3.0191, 3.0315]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9353, 2.8852, 2.9020, 2.9210]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9961, 2.9699, 2.9769, 2.9944]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9391, 2.8845, 2.9049, 2.9316]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0025, 2.9399, 2.9487, 2.9945]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3304, 3.2259, 3.2549, 3.2941]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5967, 3.4718, 3.5368, 3.5569]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5407, 3.4089, 3.4880, 3.5037]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7094, 3.5435, 3.6379, 3.6616]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6999, 3.5426, 3.6353, 3.6631]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8300, 3.6549, 3.7644, 3.7911]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7579, 3.5823, 3.6939, 3.7220]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7535, 3.5595, 3.6986, 3.7162]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7391, 3.5406, 3.6831, 3.6985]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7830, 3.5630, 3.7251, 3.7426]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8059, 3.5888, 3.7399, 3.7628]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7980, 3.5806, 3.7334, 3.7575]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7963, 3.5777, 3.7321, 3.7563]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7963, 3.5777, 3.7321, 3.7563]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7963, 3.5777, 3.7321, 3.7563]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7963, 3.5777, 3.7321, 3.7563]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7963, 3.5777, 3.7321, 3.7563]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7963, 3.5777, 3.7321, 3.7563]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7963, 3.5777, 3.7321, 3.7563]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7963, 3.5777, 3.7321, 3.7563]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7963, 3.5777, 3.7321, 3.7563]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0520, 3.0123, 3.0193, 3.0417]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0813, 3.0360, 3.0436, 3.0710]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0172, 2.9574, 2.9717, 3.0061]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0038, 2.9286, 2.9499, 2.9883]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9644, 2.8748, 2.9004, 2.9474]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9321, 2.8328, 2.8616, 2.9149]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9651, 2.8555, 2.8824, 2.9524]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0442, 2.9251, 2.9443, 3.0207]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1951, 3.0487, 3.0717, 3.1533]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1708, 3.0271, 3.0482, 3.1259]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1960, 3.0759, 3.1062, 3.1510]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3961, 3.2491, 3.3200, 3.3537]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4085, 3.2447, 3.3364, 3.3706]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6168, 3.4279, 3.5365, 3.5697]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6302, 3.4353, 3.5593, 3.5886]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7952, 3.5753, 3.7262, 3.7513]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8046, 3.5885, 3.7379, 3.7593]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7750, 3.5606, 3.7095, 3.7322]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7900, 3.5752, 3.7255, 3.7492]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7927, 3.5750, 3.7286, 3.7526]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7963, 3.5777, 3.7321, 3.7563]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7963, 3.5777, 3.7321, 3.7563]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7963, 3.5777, 3.7321, 3.7563]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7963, 3.5777, 3.7321, 3.7563]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8737, 2.8661, 2.8773, 2.8634]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1236, 3.0538, 3.1193, 3.1094]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0642, 3.0317, 3.0628, 3.0445]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9604, 2.9513, 2.9619, 2.9479]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0366, 3.0033, 3.0339, 3.0244]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2271, 3.1326, 3.2178, 3.2104]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1326, 3.0509, 3.1175, 3.1110]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0638, 3.0080, 3.0549, 3.0443]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2456, 3.1498, 3.2230, 3.2221]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2399, 3.1632, 3.2213, 3.2189]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1373, 3.0672, 3.1035, 3.1095]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1827, 3.1009, 3.1470, 3.1535]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0869, 3.0203, 3.0472, 3.0522]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1596, 3.0978, 3.1271, 3.1195]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2726, 3.1998, 3.2343, 3.2297]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2832, 3.2067, 3.2449, 3.2366]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2580, 3.1866, 3.2118, 3.2207]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2483, 3.1863, 3.2056, 3.2251]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3007, 3.2560, 3.2727, 3.2819]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2413, 3.1956, 3.2050, 3.2382]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1750, 3.1072, 3.1248, 3.1666]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1859, 3.1100, 3.1305, 3.1668]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1162, 3.0378, 3.0624, 3.1145]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0556, 2.9816, 3.0064, 3.0590]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2719, 3.2079, 3.2304, 3.2504]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3547, 3.3004, 3.3240, 3.3257]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3101, 3.2468, 3.2692, 3.2873]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2386, 3.1736, 3.1948, 3.2180]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4042, 3.3423, 3.3624, 3.3756]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3869, 3.3113, 3.3330, 3.3573]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4411, 3.3642, 3.3882, 3.4100]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5572, 3.4834, 3.5037, 3.5175]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6240, 3.5603, 3.5754, 3.5801]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6543, 3.5924, 3.6051, 3.6049]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6234, 3.5773, 3.5768, 3.6148]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8174, 3.7677, 3.7592, 3.8136]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7624, 3.7197, 3.7188, 3.7448]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.9259, 3.8646, 3.8872, 3.9068]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[4.0354, 3.9524, 3.9936, 4.0179]], grad_fn=<AddmmBackward0>)\n",
      "tensor(1.)\n",
      "tensor([[2.9572, 2.8729, 2.9462, 2.9370]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8626, 2.7944, 2.8521, 2.8540]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.5169, 2.4954, 2.5021, 2.5253]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.6811, 2.6114, 2.6439, 2.6741]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.6903, 2.6486, 2.6916, 2.6882]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8489, 2.7371, 2.8399, 2.8452]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7987, 2.7492, 2.7846, 2.7897]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8014, 2.7451, 2.7714, 2.7996]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7224, 2.6475, 2.6793, 2.7220]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.6780, 2.6311, 2.6427, 2.6841]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9837, 2.8711, 2.9701, 2.9909]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9142, 2.8364, 2.8945, 2.9160]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0679, 2.9604, 3.0598, 3.0622]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0033, 2.9174, 2.9940, 2.9958]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9874, 2.9463, 2.9632, 2.9760]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9749, 2.9511, 2.9636, 2.9538]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9869, 2.9667, 2.9764, 2.9736]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8143, 2.8008, 2.8416, 2.8341]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.6842, 2.6492, 2.6902, 2.7113]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9965, 2.8985, 3.0128, 3.0000]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9238, 2.8280, 2.9404, 2.9428]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1852, 3.0453, 3.1723, 3.1821]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2765, 3.1082, 3.2643, 3.2604]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2872, 3.1150, 3.2543, 3.2626]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2918, 3.1283, 3.2622, 3.2664]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2485, 3.0853, 3.2162, 3.2228]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2234, 3.0688, 3.1931, 3.1988]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0681, 3.1920, 3.1977]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2222, 3.0682, 3.1919, 3.1978]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8638, 2.8261, 2.8350, 2.8418]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9032, 2.8639, 2.8716, 2.8822]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8483, 2.7983, 2.8111, 2.8274]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8349, 2.7695, 2.7892, 2.8096]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7821, 2.7036, 2.7284, 2.7571]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7320, 2.6458, 2.6728, 2.7109]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7650, 2.6685, 2.6936, 2.7484]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7337, 2.6369, 2.6630, 2.7169]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7224, 2.6226, 2.6539, 2.7244]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8428, 2.7499, 2.7819, 2.8410]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8277, 2.7652, 2.7909, 2.8239]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7847, 2.7319, 2.7531, 2.7850]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7361, 2.6826, 2.7067, 2.7232]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.8877, 2.8094, 2.8401, 2.8575]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9273, 2.8726, 2.8933, 2.9014]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.9085, 2.7291, 2.7931, 3.0039]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3111, 3.1987, 3.2361, 3.2361]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3012, 3.2071, 3.2357, 3.2418]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3512, 3.2704, 3.2896, 3.3014]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4540, 3.4002, 3.4123, 3.4028]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5681, 3.5110, 3.5315, 3.5223]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4840, 3.4063, 3.4300, 3.4305]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4526, 3.3606, 3.3974, 3.3698]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4413, 3.3483, 3.3756, 3.3778]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5042, 3.4107, 3.4434, 3.4385]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6339, 3.6070, 3.6207, 3.5840]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6097, 3.5753, 3.5882, 3.5563]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7598, 3.7235, 3.7421, 3.6880]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8031, 3.7570, 3.7635, 3.7489]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8204, 3.7990, 3.8144, 3.7675]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7165, 3.6815, 3.6931, 3.6577]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8496, 3.8117, 3.8159, 3.8071]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8603, 3.8307, 3.8418, 3.8095]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6146, 3.5848, 3.6004, 3.5460]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6766, 3.6474, 3.6537, 3.6409]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6231, 3.5649, 3.5656, 3.5774]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.9557, 3.8469, 3.9066, 3.9185]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8183, 3.7489, 3.7546, 3.7883]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5199, 3.4733, 3.4790, 3.5456]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[4.2401, 4.1447, 4.1923, 4.1402]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7971, 3.7407, 3.7548, 3.7620]], grad_fn=<AddmmBackward0>)\n",
      "tensor(1.)\n",
      "tensor([[2.9100, 2.8891, 2.8967, 2.9102]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[2.7922, 2.6682, 2.6686, 2.7279]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1669, 3.1453, 3.1558, 3.1430]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0574, 3.0458, 3.0610, 3.0251]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2101, 3.1757, 3.1796, 3.1614]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5854, 3.4419, 3.5399, 3.5342]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0443, 3.0195, 3.0321, 3.0143]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.0967, 3.0773, 3.0901, 3.0669]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.1145, 3.0899, 3.1037, 3.0788]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2148, 3.1806, 3.1871, 3.1742]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2319, 3.1866, 3.1879, 3.1904]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4418, 3.3973, 3.3889, 3.3961]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4837, 3.4053, 3.3902, 3.4190]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3497, 3.3090, 3.3079, 3.3036]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4457, 3.3466, 3.3416, 3.3730]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2693, 3.1468, 3.1576, 3.2087]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4016, 3.3072, 3.3087, 3.3431]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3929, 3.2998, 3.3082, 3.3394]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4527, 3.3586, 3.3653, 3.3918]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.4386, 3.3457, 3.3469, 3.3975]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.2283, 3.1691, 3.1954, 3.2154]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3912, 3.3047, 3.3116, 3.3336]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5341, 3.4473, 3.4456, 3.4691]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5788, 3.4903, 3.4815, 3.5144]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7791, 3.6430, 3.6793, 3.7289]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7580, 3.5804, 3.6830, 3.7845]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8519, 3.6635, 3.7735, 3.7863]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7971, 3.6668, 3.7344, 3.7421]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.3788, 3.2577, 3.2934, 3.3058]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5207, 3.4385, 3.4581, 3.4664]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5139, 3.4146, 3.4404, 3.4506]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.7047, 3.6368, 3.6555, 3.6495]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6862, 3.5969, 3.6282, 3.6290]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8999, 3.7957, 3.8264, 3.8220]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6970, 3.5720, 3.6107, 3.6192]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.6953, 3.6238, 3.6457, 3.6406]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.9586, 3.9024, 3.9221, 3.8937]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5905, 3.5205, 3.5451, 3.5366]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.5395, 3.4909, 3.5046, 3.4870]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.9338, 3.8707, 3.8848, 3.8639]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[4.0710, 4.0225, 4.0298, 4.0121]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8621, 3.8134, 3.8226, 3.8155]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8482, 3.8090, 3.8182, 3.7902]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.9881, 3.9540, 3.9610, 3.9354]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[4.0795, 4.0268, 4.0326, 4.0208]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8341, 3.7961, 3.8126, 3.7728]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.9426, 3.8849, 3.8868, 3.8971]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.8230, 3.7339, 3.7328, 3.7642]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[3.9358, 3.8880, 3.9122, 3.8853]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[4.1833, 4.1705, 4.1849, 4.2153]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[7.3946, 6.7433, 6.9481, 7.5527]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[4.7362, 4.7415, 4.7299, 4.9606]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[4.9434, 4.8504, 4.9431, 5.0868]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[5.4397, 5.1801, 5.3212, 5.5139]], grad_fn=<AddmmBackward0>)\n",
      "tensor(4.)\n",
      "tensor([[1.9446, 1.9514, 1.9667, 1.9697]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.3931, 1.4326, 1.4484, 1.4783]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.5519, 1.5752, 1.5915, 1.6085]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.4721, 1.4707, 1.4887, 1.4951]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.4342, 1.4332, 1.4544, 1.4479]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.4886, 1.4824, 1.5055, 1.4949]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.5507, 1.5388, 1.5604, 1.5589]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.4221, 1.4263, 1.4452, 1.4490]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.3878, 1.3794, 1.4006, 1.3943]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.4834, 1.4763, 1.4959, 1.4956]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.5021, 1.4982, 1.5157, 1.5248]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.3279, 1.3356, 1.3528, 1.3686]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.3435, 1.3518, 1.3707, 1.3750]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.3997, 1.4074, 1.4233, 1.4437]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.3807, 1.3816, 1.3963, 1.4227]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.4278, 1.4366, 1.4523, 1.4716]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.3607, 1.3609, 1.3793, 1.3986]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.3073, 1.3197, 1.3413, 1.3445]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.3664, 1.3723, 1.3907, 1.3987]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.3965, 1.4032, 1.4201, 1.4342]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.3621, 1.3749, 1.3907, 1.4136]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.5439, 1.5579, 1.5731, 1.5918]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.4399, 1.4498, 1.4640, 1.4913]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.5757, 1.5803, 1.5924, 1.6223]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.5738, 1.5792, 1.5913, 1.6228]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.5927, 1.5933, 1.6056, 1.6350]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.4557, 1.4729, 1.4875, 1.5152]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.4319, 1.4498, 1.4646, 1.4911]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.3207, 1.3466, 1.3635, 1.3859]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.3328, 1.3591, 1.3758, 1.3987]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.2883, 1.3159, 1.3334, 1.3545]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.2909, 1.3184, 1.3358, 1.3570]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.2909, 1.3184, 1.3358, 1.3570]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.2909, 1.3184, 1.3358, 1.3570]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.)\n",
      "tensor([[1.2909, 1.3184, 1.3358, 1.3570]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a_zero_count = 0\n",
    "s = env.reset(seed=42)\n",
    "Q = dqn(s.permute(2, 0, 1).unsqueeze(0))\n",
    "print(Q)\n",
    "a = Q.argmax().item()\n",
    "terminated = False\n",
    "while terminated==False:\n",
    "    s_prime, r, terminated =  env.step(a)\n",
    "    print(r)\n",
    "    s_prime = preprocess(s_prime)\n",
    "    s_prime = torch.cat([s, s_prime.unsqueeze(2)], 2)[:, :, -4:]\n",
    "    s = s_prime\n",
    "    Q = dqn(s.permute(2, 0, 1).unsqueeze(0))\n",
    "    a = Q.argmax().item()\n",
    "    \n",
    "    \n",
    "    print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe5aa7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "s = env.reset(seed=42)\n",
    "terminated=False\n",
    "while terminated == False:\n",
    "    a = dqn.select_next_action(s.permute(2, 0, 1).unsqueeze(0).float(), 0.1)\n",
    "    s_prime, r, terminated = env.step(a)\n",
    "    images.append(s_prime.clone().long().numpy())\n",
    "    s_prime = preprocess(s_prime)\n",
    "    s_prime = torch.cat([s, s_prime.unsqueeze(2)], 2)[:, :, -4:]\n",
    "    s = s_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7264e417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "WRITERS[\"opencv\"](iter(images), \"test.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77e9996b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-macosx_10_16_x86_64.whl (55.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in ./docker/predict/venv/lib/python3.8/site-packages (from opencv-python) (1.23.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "242269a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, Iterator, Literal, Optional\n",
    "\n",
    "import cv2\n",
    "import ffmpeg\n",
    "import matplotlib.animation as ani\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def as_uint8(arr: np.ndarray) -> np.ndarray:\n",
    "    if np.issubdtype(arr.dtype, np.integer):\n",
    "        return arr.astype(np.uint8)\n",
    "    if np.issubdtype(arr.dtype, np.floating):\n",
    "        return (arr * 255).round().astype(np.uint8)\n",
    "    raise NotImplementedError(f\"Unsupported dtype: {arr.dtype}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VideoProps:\n",
    "    frame_width: int\n",
    "    frame_height: int\n",
    "    frame_count: int\n",
    "    fps: int\n",
    "\n",
    "    @classmethod\n",
    "    def from_file_opencv(cls, fpath: str) -> \"VideoProps\":\n",
    "        cap = cv2.VideoCapture(str(fpath))\n",
    "\n",
    "        return cls(\n",
    "            frame_width=int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "            frame_height=int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
    "            frame_count=int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "            fps=int(cap.get(cv2.CAP_PROP_FPS)),\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_file_ffmpeg(cls, fpath: str) -> \"VideoProps\":\n",
    "        probe = ffmpeg.probe(str(fpath))\n",
    "\n",
    "        for stream in probe[\"streams\"]:\n",
    "            if stream[\"codec_type\"] == \"video\":\n",
    "                width, height, count = stream[\"width\"], stream[\"height\"], int(stream[\"nb_frames\"])\n",
    "                fps_num, fps_denom = stream[\"r_frame_rate\"].split(\"/\")\n",
    "                assert fps_denom == \"1\", f\"Unexpected frame rate: {stream['r_frame_rate']}\"\n",
    "                fps = int(fps_num)\n",
    "                return cls(\n",
    "                    frame_width=width,\n",
    "                    frame_height=height,\n",
    "                    frame_count=count,\n",
    "                    fps=fps,\n",
    "                )\n",
    "\n",
    "        raise ValueError(f\"Could not parse video properties from video in {fpath}\")\n",
    "\n",
    "\n",
    "def read_video_ffmpeg(\n",
    "    in_file: str,\n",
    "    output_fmt: str = \"rgb24\",\n",
    "    channels: int = 3,\n",
    ") -> Iterator[np.ndarray]:\n",
    "    \"\"\"Function that reads a video to a stream of numpy arrays using FFMPEG.\n",
    "\n",
    "    Args:\n",
    "        in_file: The input video to read\n",
    "        output_fmt: The output image format\n",
    "        channels: Number of output channels for each video frame\n",
    "\n",
    "    Yields:\n",
    "        Frames from the video as numpy arrays with shape (H, W, C)\n",
    "    \"\"\"\n",
    "\n",
    "    props = VideoProps.from_file_ffmpeg(in_file)\n",
    "\n",
    "    stream = ffmpeg.input(str(in_file))\n",
    "    stream = ffmpeg.output(stream, \"pipe:\", format=\"rawvideo\", pix_fmt=output_fmt, r=props.fps)\n",
    "    stream = ffmpeg.run_async(stream, pipe_stdout=True)\n",
    "\n",
    "    while True:\n",
    "        in_bytes = stream.stdout.read(props.frame_width * props.frame_height * channels)\n",
    "        if not in_bytes:\n",
    "            break\n",
    "        yield np.frombuffer(in_bytes, np.uint8).reshape((props.frame_height, props.frame_width, channels))\n",
    "\n",
    "    stream.stdout.close()\n",
    "    stream.wait()\n",
    "\n",
    "\n",
    "def read_video_opencv(in_file: str) -> Iterator[np.ndarray]:\n",
    "    \"\"\"Reads a video as a stream using OpenCV.\n",
    "\n",
    "    Args:\n",
    "        in_file: The input video to read\n",
    "\n",
    "    Yields:\n",
    "        Frames from the video as numpy arrays with shape (H, W, C)\n",
    "    \"\"\"\n",
    "\n",
    "    cap = cv2.VideoCapture(str(in_file))\n",
    "\n",
    "    while True:\n",
    "        ret, buffer = cap.read()\n",
    "        if not ret:\n",
    "            cap.release()\n",
    "            return\n",
    "        yield buffer\n",
    "\n",
    "\n",
    "def write_video_opencv(\n",
    "    itr: Iterator[np.ndarray],\n",
    "    out_file: str,\n",
    "    fps: int = 30,\n",
    "    codec: str = \"MP4V\",\n",
    ") -> None:\n",
    "    \"\"\"Function that writes a video from a stream of numpy arrays using OpenCV.\n",
    "\n",
    "    Args:\n",
    "        itr: The image iterator, yielding images with shape (H, W, C).\n",
    "        out_file: The path to the output file.\n",
    "        fps: Frames per second for the video.\n",
    "        codec: FourCC code specifying OpenCV video codec type. Examples are\n",
    "            MPEG, MP4V, DIVX, AVC1, H236.\n",
    "    \"\"\"\n",
    "\n",
    "    first_img = next(itr)\n",
    "    height, width, _ = first_img.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "    stream = cv2.VideoWriter(str(out_file), fourcc, fps, (width, height))\n",
    "\n",
    "    def write_frame(img: np.ndarray) -> None:\n",
    "        stream.write(as_uint8(img))\n",
    "\n",
    "    write_frame(first_img)\n",
    "    for img in itr:\n",
    "        write_frame(img)\n",
    "\n",
    "    stream.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def write_video_ffmpeg(\n",
    "    itr: Iterator[np.ndarray],\n",
    "    out_file: str,\n",
    "    fps: int = 30,\n",
    "    out_fps: int = 30,\n",
    "    vcodec: str = \"libx264\",\n",
    "    input_fmt: str = \"rgb24\",\n",
    "    output_fmt: str = \"yuv420p\",\n",
    ") -> None:\n",
    "    \"\"\"Function that writes an video from a stream of numpy arrays using FFMPEG.\n",
    "\n",
    "    Args:\n",
    "        itr: The image iterator, yielding images with shape (H, W, C).\n",
    "        out_file: The path to the output file.\n",
    "        fps: Frames per second for the video.\n",
    "        out_fps: Frames per second for the saved video.\n",
    "        vcodec: The video codec to use for the output video\n",
    "        input_fmt: The input image format\n",
    "        output_fmt: The output image format\n",
    "    \"\"\"\n",
    "\n",
    "    first_img = next(itr)\n",
    "    height, width, _ = first_img.shape\n",
    "\n",
    "    stream = ffmpeg.input(\"pipe:\", format=\"rawvideo\", pix_fmt=input_fmt, s=f\"{width}x{height}\", r=fps)\n",
    "    stream = ffmpeg.output(stream, str(out_file), pix_fmt=output_fmt, vcodec=vcodec, r=out_fps)\n",
    "    stream = ffmpeg.overwrite_output(stream)\n",
    "    stream = ffmpeg.run_async(stream, pipe_stdin=True)\n",
    "\n",
    "    def write_frame(img: np.ndarray) -> None:\n",
    "        stream.stdin.write(as_uint8(img).tobytes())\n",
    "\n",
    "    # Writes all the video frames to the file.\n",
    "    write_frame(first_img)\n",
    "    for img in itr:\n",
    "        write_frame(img)\n",
    "\n",
    "    stream.stdin.close()\n",
    "    stream.wait()\n",
    "\n",
    "\n",
    "def write_video_matplotlib(\n",
    "    itr: Iterator[np.ndarray],\n",
    "    out_file: str,\n",
    "    dpi: int = 50,\n",
    "    fps: int = 30,\n",
    "    title: str = \"Video\",\n",
    "    comment: Optional[str] = None,\n",
    "    writer: str = \"ffmpeg\",\n",
    ") -> None:\n",
    "    \"\"\"Function that writes an video from a stream of input tensors.\n",
    "\n",
    "    Args:\n",
    "        itr: The image iterator, yielding images with shape (H, W, C).\n",
    "        out_file: The path to the output file.\n",
    "        dpi: Dots per inch for output image.\n",
    "        fps: Frames per second for the video.\n",
    "        title: Title for the video metadata.\n",
    "        comment: Comment for the video metadata.\n",
    "        writer: The Matplotlib video writer to use (if you use the\n",
    "            default one, make sure you have `ffmpeg` installed on your\n",
    "            system).\n",
    "    \"\"\"\n",
    "\n",
    "    first_img = next(itr)\n",
    "    height, width, _ = first_img.shape\n",
    "    fig, ax = plt.subplots(figsize=(width / dpi, height / dpi))\n",
    "\n",
    "    # Ensures that there's no extra space around the image.\n",
    "    fig.subplots_adjust(\n",
    "        left=0,\n",
    "        bottom=0,\n",
    "        right=1,\n",
    "        top=1,\n",
    "        wspace=None,\n",
    "        hspace=None,\n",
    "    )\n",
    "\n",
    "    # Creates the writer with the given metadata.\n",
    "    writer_obj = ani.writers[writer]\n",
    "    metadata = {\n",
    "        \"title\": title,\n",
    "        \"artist\": __name__,\n",
    "        \"comment\": comment,\n",
    "    }\n",
    "    mpl_writer = writer_obj(\n",
    "        fps=fps,\n",
    "        metadata={k: v for k, v in metadata.items() if v is not None},\n",
    "    )\n",
    "\n",
    "    with mpl_writer.saving(fig, out_file, dpi=dpi):\n",
    "        im = ax.imshow(as_uint8(first_img), interpolation=\"nearest\")\n",
    "        mpl_writer.grab_frame()\n",
    "\n",
    "        for img in itr:\n",
    "            im.set_data(as_uint8(img))\n",
    "            mpl_writer.grab_frame()\n",
    "\n",
    "\n",
    "Reader = Literal[\"ffmpeg\", \"opencv\"]\n",
    "Writer = Literal[\"ffmpeg\", \"matplotlib\", \"opencv\"]\n",
    "\n",
    "READERS: Dict[Reader, Callable[[str], Iterator[np.ndarray]]] = {\n",
    "    \"ffmpeg\": read_video_ffmpeg,\n",
    "    \"opencv\": read_video_opencv,\n",
    "}\n",
    "\n",
    "WRITERS: Dict[Writer, Callable[[Iterator[np.ndarray], str], None]] = {\n",
    "    \"ffmpeg\": write_video_ffmpeg,\n",
    "    \"matplotlib\": write_video_matplotlib,\n",
    "    \"opencv\": write_video_opencv,\n",
    "}\n",
    "\n",
    "# Remove the FFMPEG reader and writer if FFMPEG is not available in the system.\n",
    "if not shutil.which(\"ffmpeg\"):\n",
    "    READERS.pop(\"ffmpeg\")\n",
    "    WRITERS.pop(\"ffmpeg\")\n",
    "    WRITERS.pop(\"matplotlib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5aa46a",
   "metadata": {
    "id": "1c5aa46a"
   },
   "source": [
    "## Improve Performance Efficiency\n",
    "An issue we are facing is that the code currently makes multiple forward passes through the network as we are fetching values for each individual training example. This makes the code easier to understand but very slow to run. We will change that so that batches of data will be passed to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db444b",
   "metadata": {
    "id": "74db444b",
    "outputId": "ceefc037-8bca-4bc0-c0a4-5dc59b38ec8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<magic-timeit>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/var/folders/8w/2dpk8lsx6hl861tqrs31bxb80000gn/T/ipykernel_60609/583668238.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s)\n",
      "/var/folders/8w/2dpk8lsx6hl861tqrs31bxb80000gn/T/ipykernel_60609/583668238.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s_prime = torch.tensor(s_prime)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 episode_loss 19.43286417717536, reward 1.0, steps 196\n",
      "episode 1 episode_loss 0.08061189765794552, reward 0.0, steps 133\n",
      "episode 1 episode_loss 0.08898574351405841, reward 1.0, steps 161\n",
      "episode 1 episode_loss 0.26165696167299757, reward 2.0, steps 223\n",
      "episode 1 episode_loss 0.5946118091815151, reward 5.0, steps 300\n",
      "episode 1 episode_loss 0.23256916616810486, reward 1.0, steps 167\n",
      "episode 1 episode_loss 0.20791195419587893, reward 1.0, steps 185\n",
      "episode 1 episode_loss 0.1303796372958459, reward 0.0, steps 145\n",
      "28.3 s ± 8.7 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "N = 1000 #replay memory max size\n",
    "bs = 32\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "\n",
    "M = 1\n",
    "steps = 0\n",
    "for i in range(M):\n",
    "    terminated = False\n",
    "    s, info = reset(env)\n",
    "    s = torch.tensor(s)\n",
    "    episode_loss = 0\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    while terminated == False:\n",
    "\n",
    "        a = dqn.select_next_action(s, epsilon)\n",
    "        s_prime, r, terminated, truncated, info = step(env, a)\n",
    "        s_prime = preprocess(s_prime)\n",
    "        s_prime = torch.cat([s, s_prime.unsqueeze(2)], 2)[:, :, -4:]\n",
    "        replay_memory.append((s, a, r, s_prime, terminated))\n",
    "\n",
    "        episode_reward += r.item()\n",
    "        episode_length += 1\n",
    "\n",
    "        batch = replay_memory.sample(bs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat, y = get_batch(dqn, batch)\n",
    "\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "        episode_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "        torch.nn.utils.clip_grad_value_(dqn.parameters(), 100)\n",
    "        if epsilon > 0.05 :\n",
    "            epsilon -= (1 / 5000)\n",
    "        s = s_prime\n",
    "\n",
    "    episode_lengths.append(episode_length)\n",
    "\n",
    "    # if i % 20 == 0:\n",
    "    print(f'episode {i+1} episode_loss {episode_loss}, reward {episode_reward}, steps {steps}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c2024",
   "metadata": {
    "id": "394c2024",
    "outputId": "14a80e64-3e2f-467b-dbf5-e549ba81ec0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184.5"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_steps_per_episode = (134 + 281 + 170 + 134 + 243 + 129 + 203 + 182)/8\n",
    "mean_steps_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c703d",
   "metadata": {
    "id": "718c703d"
   },
   "outputs": [],
   "source": [
    "bs = 32\n",
    "class EfficientAtariDQN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_actions):\n",
    "        super(EfficientAtariDQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, 8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*9*9, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def forward(self, s):\n",
    "        return self.conv(s.float())\n",
    "\n",
    "    def select_next_action(self, s, epsilon):\n",
    "        use_greedy = np.random.binomial(1, 1-epsilon)\n",
    "        if use_greedy:\n",
    "            a = self(s).argmax().item()\n",
    "        else:\n",
    "            a = np.random.choice(self.n_actions)\n",
    "        return a\n",
    "\n",
    "dqn = EfficientAtariDQN(in_dim=env.observation_space.shape[0], hidden_dim=9, n_actions=env.action_space.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4935c",
   "metadata": {
    "id": "99c4935c"
   },
   "outputs": [],
   "source": [
    "def atari_collate(batch):\n",
    "    s_j, a_j, r_j, s_prime_j, terminated_j = list(zip(*batch))\n",
    "    return torch.stack(s_j).permute(0, 3, 1, 2), torch.tensor(a_j), torch.tensor(r_j), torch.stack(s_prime_j).permute(0, 3, 1, 2), (~torch.tensor(terminated_j)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e45740",
   "metadata": {
    "id": "e4e45740",
    "outputId": "705ab84a-0f96-42f2-a8e2-6fe1708b9cbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 µs ± 2.02 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch = replay_memory.sample(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d61cf84",
   "metadata": {
    "id": "6d61cf84",
    "outputId": "08eee888-8ac0-4767-d1f5-9afb61a72f40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62500.0"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000000 / 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eda7b0",
   "metadata": {
    "id": "26eda7b0",
    "outputId": "634269f0-86bb-4b9e-d435-3c913e268d4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8w/2dpk8lsx6hl861tqrs31bxb80000gn/T/ipykernel_60609/3614971170.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s)\n",
      "/var/folders/8w/2dpk8lsx6hl861tqrs31bxb80000gn/T/ipykernel_60609/583668238.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s)\n",
      "/var/folders/8w/2dpk8lsx6hl861tqrs31bxb80000gn/T/ipykernel_60609/583668238.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s_prime = torch.tensor(s_prime)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 episode_loss 482.0094321966171, reward 3.0, steps 272, epoch 0.00544\n",
      "episode 101 episode_loss 322.11623126268387, reward 2.0, steps 21058, epoch 0.42116\n",
      "episode 201 episode_loss 215.76791804283857, reward 8.0, steps 48889, epoch 0.97778\n",
      "episode 301 episode_loss 119.2069219830446, reward 7.0, steps 89477, epoch 1.78954\n",
      "episode 401 episode_loss 99.53891976736486, reward 8.0, steps 134177, epoch 2.68354\n",
      "episode 501 episode_loss 60.76320995064452, reward 7.0, steps 176072, epoch 3.52144\n",
      "episode 601 episode_loss 89.25185312365647, reward 7.0, steps 219691, epoch 4.39382\n",
      "episode 701 episode_loss 63.48463975416962, reward 4.0, steps 262570, epoch 5.2514\n",
      "episode 801 episode_loss 86.89205625501927, reward 3.0, steps 305018, epoch 6.10036\n",
      "episode 901 episode_loss 104.9817000987241, reward 7.0, steps 349154, epoch 6.98308\n",
      "episode 1001 episode_loss 97.93874165776651, reward 11.0, steps 391892, epoch 7.83784\n",
      "episode 1101 episode_loss 76.31025852193125, reward 7.0, steps 434048, epoch 8.68096\n",
      "episode 1201 episode_loss 148.8159403713653, reward 7.0, steps 476821, epoch 9.53642\n",
      "episode 1301 episode_loss 69.77878785296343, reward 1.0, steps 520619, epoch 10.41238\n",
      "episode 1401 episode_loss 106.68529581662733, reward 11.0, steps 562441, epoch 11.24882\n",
      "episode 1501 episode_loss 118.52021295041777, reward 7.0, steps 606020, epoch 12.1204\n",
      "episode 1601 episode_loss 64.24022578995209, reward 7.0, steps 648801, epoch 12.97602\n",
      "episode 1701 episode_loss 112.87910554383416, reward 7.0, steps 692098, epoch 13.84196\n",
      "episode 1801 episode_loss 79.37506981205661, reward 7.0, steps 736195, epoch 14.7239\n",
      "episode 1901 episode_loss 99.0631368541508, reward 11.0, steps 778806, epoch 15.57612\n"
     ]
    }
   ],
   "source": [
    "N = 62500 #replay memory max size\n",
    "bs = 32\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "\n",
    "M = 2000\n",
    "steps = 0\n",
    "epoch = 1\n",
    "for i in range(M):\n",
    "    terminated = False\n",
    "    s, info = reset(env)\n",
    "    s = torch.tensor(s)\n",
    "    episode_loss = 0\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    while terminated == False:\n",
    "\n",
    "        a = dqn.select_next_action(s.permute(2, 0, 1).unsqueeze(0), epsilon)\n",
    "        s_prime, r, terminated, truncated, info = step(env, a)\n",
    "        s_prime = preprocess(s_prime)\n",
    "        s_prime = torch.cat([s, s_prime.unsqueeze(2)], 2)[:, :, -4:]\n",
    "        replay_memory.append((s, a, r, s_prime, terminated))\n",
    "\n",
    "        episode_reward += r.item()\n",
    "        episode_length += 1\n",
    "\n",
    "        batch = replay_memory.sample(bs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat, y = get_batch_efficient(dqn, batch, atari_collate)\n",
    "\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "        episode_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "        torch.nn.utils.clip_grad_value_(dqn.parameters(), 100)\n",
    "        epsilon = get_epsilon(epsilon)\n",
    "        s = s_prime\n",
    "\n",
    "    episode_lengths.append(episode_length)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        epoch = steps / 50000\n",
    "        print(f'episode {i+1} episode_loss {episode_loss}, reward {episode_reward}, steps {steps}, epoch {epoch}')\n",
    "        torch.save(dqn, 'breakout.pt')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9530d8",
   "metadata": {
    "id": "5c9530d8",
    "outputId": "e5bf8216-8db4-49d2-d51b-779d4d335f58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8w/2dpk8lsx6hl861tqrs31bxb80000gn/T/ipykernel_60609/4143087600.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s = torch.tensor(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 {'lives': 5, 'episode_frame_number': 16, 'frame_number': 16}\n",
      "3 {'lives': 5, 'episode_frame_number': 20, 'frame_number': 20}\n",
      "3 {'lives': 5, 'episode_frame_number': 24, 'frame_number': 24}\n",
      "3 {'lives': 5, 'episode_frame_number': 28, 'frame_number': 28}\n",
      "3 {'lives': 5, 'episode_frame_number': 32, 'frame_number': 32}\n",
      "3 {'lives': 5, 'episode_frame_number': 36, 'frame_number': 36}\n",
      "3 {'lives': 5, 'episode_frame_number': 40, 'frame_number': 40}\n",
      "3 {'lives': 5, 'episode_frame_number': 44, 'frame_number': 44}\n",
      "3 {'lives': 5, 'episode_frame_number': 48, 'frame_number': 48}\n",
      "3 {'lives': 5, 'episode_frame_number': 52, 'frame_number': 52}\n",
      "3 {'lives': 5, 'episode_frame_number': 56, 'frame_number': 56}\n",
      "3 {'lives': 5, 'episode_frame_number': 60, 'frame_number': 60}\n",
      "3 {'lives': 5, 'episode_frame_number': 64, 'frame_number': 64}\n",
      "3 {'lives': 5, 'episode_frame_number': 68, 'frame_number': 68}\n",
      "0 {'lives': 5, 'episode_frame_number': 72, 'frame_number': 72}\n",
      "3 {'lives': 5, 'episode_frame_number': 76, 'frame_number': 76}\n",
      "3 {'lives': 5, 'episode_frame_number': 80, 'frame_number': 80}\n",
      "3 {'lives': 5, 'episode_frame_number': 84, 'frame_number': 84}\n",
      "3 {'lives': 5, 'episode_frame_number': 88, 'frame_number': 88}\n",
      "3 {'lives': 5, 'episode_frame_number': 92, 'frame_number': 92}\n",
      "3 {'lives': 5, 'episode_frame_number': 96, 'frame_number': 96}\n",
      "3 {'lives': 5, 'episode_frame_number': 100, 'frame_number': 100}\n",
      "3 {'lives': 5, 'episode_frame_number': 104, 'frame_number': 104}\n",
      "3 {'lives': 5, 'episode_frame_number': 108, 'frame_number': 108}\n",
      "3 {'lives': 5, 'episode_frame_number': 112, 'frame_number': 112}\n",
      "3 {'lives': 5, 'episode_frame_number': 116, 'frame_number': 116}\n",
      "3 {'lives': 5, 'episode_frame_number': 120, 'frame_number': 120}\n",
      "3 {'lives': 5, 'episode_frame_number': 124, 'frame_number': 124}\n",
      "3 {'lives': 5, 'episode_frame_number': 128, 'frame_number': 128}\n",
      "3 {'lives': 5, 'episode_frame_number': 132, 'frame_number': 132}\n",
      "3 {'lives': 5, 'episode_frame_number': 136, 'frame_number': 136}\n",
      "3 {'lives': 5, 'episode_frame_number': 140, 'frame_number': 140}\n",
      "3 {'lives': 5, 'episode_frame_number': 144, 'frame_number': 144}\n",
      "3 {'lives': 5, 'episode_frame_number': 148, 'frame_number': 148}\n",
      "3 {'lives': 5, 'episode_frame_number': 152, 'frame_number': 152}\n",
      "3 {'lives': 5, 'episode_frame_number': 156, 'frame_number': 156}\n",
      "3 {'lives': 5, 'episode_frame_number': 160, 'frame_number': 160}\n",
      "3 {'lives': 5, 'episode_frame_number': 164, 'frame_number': 164}\n",
      "3 {'lives': 5, 'episode_frame_number': 168, 'frame_number': 168}\n",
      "3 {'lives': 5, 'episode_frame_number': 172, 'frame_number': 172}\n",
      "3 {'lives': 5, 'episode_frame_number': 176, 'frame_number': 176}\n",
      "3 {'lives': 5, 'episode_frame_number': 180, 'frame_number': 180}\n",
      "3 {'lives': 5, 'episode_frame_number': 184, 'frame_number': 184}\n",
      "3 {'lives': 5, 'episode_frame_number': 188, 'frame_number': 188}\n",
      "3 {'lives': 5, 'episode_frame_number': 192, 'frame_number': 192}\n",
      "3 {'lives': 5, 'episode_frame_number': 196, 'frame_number': 196}\n",
      "3 {'lives': 5, 'episode_frame_number': 200, 'frame_number': 200}\n",
      "3 {'lives': 5, 'episode_frame_number': 204, 'frame_number': 204}\n",
      "3 {'lives': 5, 'episode_frame_number': 208, 'frame_number': 208}\n",
      "3 {'lives': 5, 'episode_frame_number': 212, 'frame_number': 212}\n",
      "3 {'lives': 5, 'episode_frame_number': 216, 'frame_number': 216}\n",
      "3 {'lives': 5, 'episode_frame_number': 220, 'frame_number': 220}\n",
      "3 {'lives': 5, 'episode_frame_number': 224, 'frame_number': 224}\n",
      "3 {'lives': 5, 'episode_frame_number': 228, 'frame_number': 228}\n",
      "3 {'lives': 5, 'episode_frame_number': 232, 'frame_number': 232}\n",
      "3 {'lives': 5, 'episode_frame_number': 236, 'frame_number': 236}\n",
      "2 {'lives': 5, 'episode_frame_number': 240, 'frame_number': 240}\n",
      "3 {'lives': 5, 'episode_frame_number': 244, 'frame_number': 244}\n",
      "3 {'lives': 5, 'episode_frame_number': 248, 'frame_number': 248}\n",
      "3 {'lives': 5, 'episode_frame_number': 252, 'frame_number': 252}\n",
      "3 {'lives': 5, 'episode_frame_number': 256, 'frame_number': 256}\n",
      "0 {'lives': 5, 'episode_frame_number': 260, 'frame_number': 260}\n",
      "3 {'lives': 5, 'episode_frame_number': 264, 'frame_number': 264}\n",
      "3 {'lives': 5, 'episode_frame_number': 268, 'frame_number': 268}\n",
      "3 {'lives': 5, 'episode_frame_number': 272, 'frame_number': 272}\n",
      "3 {'lives': 5, 'episode_frame_number': 276, 'frame_number': 276}\n",
      "3 {'lives': 5, 'episode_frame_number': 280, 'frame_number': 280}\n",
      "3 {'lives': 5, 'episode_frame_number': 284, 'frame_number': 284}\n",
      "3 {'lives': 5, 'episode_frame_number': 288, 'frame_number': 288}\n",
      "2 {'lives': 5, 'episode_frame_number': 292, 'frame_number': 292}\n",
      "3 {'lives': 5, 'episode_frame_number': 296, 'frame_number': 296}\n",
      "3 {'lives': 5, 'episode_frame_number': 300, 'frame_number': 300}\n",
      "3 {'lives': 5, 'episode_frame_number': 304, 'frame_number': 304}\n",
      "3 {'lives': 5, 'episode_frame_number': 308, 'frame_number': 308}\n",
      "3 {'lives': 5, 'episode_frame_number': 312, 'frame_number': 312}\n",
      "3 {'lives': 5, 'episode_frame_number': 316, 'frame_number': 316}\n",
      "3 {'lives': 5, 'episode_frame_number': 320, 'frame_number': 320}\n",
      "3 {'lives': 5, 'episode_frame_number': 324, 'frame_number': 324}\n",
      "3 {'lives': 5, 'episode_frame_number': 328, 'frame_number': 328}\n",
      "3 {'lives': 5, 'episode_frame_number': 332, 'frame_number': 332}\n",
      "3 {'lives': 5, 'episode_frame_number': 336, 'frame_number': 336}\n",
      "3 {'lives': 5, 'episode_frame_number': 340, 'frame_number': 340}\n",
      "3 {'lives': 5, 'episode_frame_number': 344, 'frame_number': 344}\n",
      "3 {'lives': 5, 'episode_frame_number': 348, 'frame_number': 348}\n",
      "3 {'lives': 5, 'episode_frame_number': 352, 'frame_number': 352}\n",
      "3 {'lives': 5, 'episode_frame_number': 356, 'frame_number': 356}\n",
      "3 {'lives': 5, 'episode_frame_number': 360, 'frame_number': 360}\n",
      "3 {'lives': 5, 'episode_frame_number': 364, 'frame_number': 364}\n",
      "3 {'lives': 5, 'episode_frame_number': 368, 'frame_number': 368}\n",
      "3 {'lives': 5, 'episode_frame_number': 372, 'frame_number': 372}\n",
      "3 {'lives': 5, 'episode_frame_number': 376, 'frame_number': 376}\n",
      "3 {'lives': 5, 'episode_frame_number': 380, 'frame_number': 380}\n",
      "3 {'lives': 5, 'episode_frame_number': 384, 'frame_number': 384}\n",
      "3 {'lives': 5, 'episode_frame_number': 388, 'frame_number': 388}\n",
      "3 {'lives': 5, 'episode_frame_number': 392, 'frame_number': 392}\n",
      "3 {'lives': 5, 'episode_frame_number': 396, 'frame_number': 396}\n",
      "3 {'lives': 5, 'episode_frame_number': 400, 'frame_number': 400}\n",
      "3 {'lives': 5, 'episode_frame_number': 404, 'frame_number': 404}\n",
      "2 {'lives': 5, 'episode_frame_number': 408, 'frame_number': 408}\n",
      "3 {'lives': 5, 'episode_frame_number': 412, 'frame_number': 412}\n",
      "3 {'lives': 5, 'episode_frame_number': 416, 'frame_number': 416}\n",
      "3 {'lives': 5, 'episode_frame_number': 420, 'frame_number': 420}\n",
      "3 {'lives': 5, 'episode_frame_number': 424, 'frame_number': 424}\n",
      "3 {'lives': 5, 'episode_frame_number': 428, 'frame_number': 428}\n",
      "3 {'lives': 5, 'episode_frame_number': 432, 'frame_number': 432}\n",
      "3 {'lives': 5, 'episode_frame_number': 436, 'frame_number': 436}\n",
      "3 {'lives': 5, 'episode_frame_number': 440, 'frame_number': 440}\n",
      "3 {'lives': 5, 'episode_frame_number': 444, 'frame_number': 444}\n",
      "3 {'lives': 5, 'episode_frame_number': 448, 'frame_number': 448}\n",
      "3 {'lives': 5, 'episode_frame_number': 452, 'frame_number': 452}\n",
      "3 {'lives': 5, 'episode_frame_number': 456, 'frame_number': 456}\n",
      "3 {'lives': 5, 'episode_frame_number': 460, 'frame_number': 460}\n",
      "0 {'lives': 5, 'episode_frame_number': 464, 'frame_number': 464}\n",
      "3 {'lives': 5, 'episode_frame_number': 468, 'frame_number': 468}\n",
      "3 {'lives': 5, 'episode_frame_number': 472, 'frame_number': 472}\n",
      "3 {'lives': 5, 'episode_frame_number': 476, 'frame_number': 476}\n",
      "3 {'lives': 5, 'episode_frame_number': 480, 'frame_number': 480}\n",
      "2 {'lives': 5, 'episode_frame_number': 484, 'frame_number': 484}\n",
      "3 {'lives': 5, 'episode_frame_number': 488, 'frame_number': 488}\n",
      "3 {'lives': 5, 'episode_frame_number': 492, 'frame_number': 492}\n",
      "3 {'lives': 5, 'episode_frame_number': 496, 'frame_number': 496}\n",
      "3 {'lives': 5, 'episode_frame_number': 500, 'frame_number': 500}\n",
      "1 {'lives': 5, 'episode_frame_number': 504, 'frame_number': 504}\n",
      "3 {'lives': 5, 'episode_frame_number': 508, 'frame_number': 508}\n",
      "3 {'lives': 5, 'episode_frame_number': 512, 'frame_number': 512}\n",
      "3 {'lives': 5, 'episode_frame_number': 516, 'frame_number': 516}\n",
      "3 {'lives': 5, 'episode_frame_number': 520, 'frame_number': 520}\n",
      "3 {'lives': 5, 'episode_frame_number': 524, 'frame_number': 524}\n",
      "3 {'lives': 5, 'episode_frame_number': 528, 'frame_number': 528}\n",
      "3 {'lives': 5, 'episode_frame_number': 532, 'frame_number': 532}\n",
      "3 {'lives': 5, 'episode_frame_number': 536, 'frame_number': 536}\n",
      "3 {'lives': 5, 'episode_frame_number': 540, 'frame_number': 540}\n",
      "2 {'lives': 5, 'episode_frame_number': 544, 'frame_number': 544}\n",
      "3 {'lives': 5, 'episode_frame_number': 548, 'frame_number': 548}\n",
      "3 {'lives': 5, 'episode_frame_number': 552, 'frame_number': 552}\n",
      "3 {'lives': 5, 'episode_frame_number': 556, 'frame_number': 556}\n",
      "3 {'lives': 5, 'episode_frame_number': 560, 'frame_number': 560}\n",
      "3 {'lives': 5, 'episode_frame_number': 564, 'frame_number': 564}\n",
      "3 {'lives': 5, 'episode_frame_number': 568, 'frame_number': 568}\n",
      "3 {'lives': 5, 'episode_frame_number': 572, 'frame_number': 572}\n",
      "3 {'lives': 5, 'episode_frame_number': 576, 'frame_number': 576}\n",
      "3 {'lives': 5, 'episode_frame_number': 580, 'frame_number': 580}\n",
      "3 {'lives': 5, 'episode_frame_number': 584, 'frame_number': 584}\n",
      "3 {'lives': 5, 'episode_frame_number': 588, 'frame_number': 588}\n",
      "3 {'lives': 5, 'episode_frame_number': 592, 'frame_number': 592}\n",
      "3 {'lives': 5, 'episode_frame_number': 596, 'frame_number': 596}\n",
      "3 {'lives': 5, 'episode_frame_number': 600, 'frame_number': 600}\n",
      "0 {'lives': 5, 'episode_frame_number': 604, 'frame_number': 604}\n",
      "3 {'lives': 5, 'episode_frame_number': 608, 'frame_number': 608}\n",
      "3 {'lives': 5, 'episode_frame_number': 612, 'frame_number': 612}\n",
      "3 {'lives': 5, 'episode_frame_number': 616, 'frame_number': 616}\n",
      "3 {'lives': 5, 'episode_frame_number': 620, 'frame_number': 620}\n",
      "3 {'lives': 5, 'episode_frame_number': 624, 'frame_number': 624}\n",
      "3 {'lives': 5, 'episode_frame_number': 628, 'frame_number': 628}\n",
      "3 {'lives': 5, 'episode_frame_number': 632, 'frame_number': 632}\n",
      "3 {'lives': 5, 'episode_frame_number': 636, 'frame_number': 636}\n",
      "3 {'lives': 5, 'episode_frame_number': 640, 'frame_number': 640}\n",
      "3 {'lives': 5, 'episode_frame_number': 644, 'frame_number': 644}\n",
      "3 {'lives': 5, 'episode_frame_number': 648, 'frame_number': 648}\n",
      "3 {'lives': 5, 'episode_frame_number': 652, 'frame_number': 652}\n",
      "3 {'lives': 5, 'episode_frame_number': 656, 'frame_number': 656}\n",
      "3 {'lives': 5, 'episode_frame_number': 660, 'frame_number': 660}\n",
      "3 {'lives': 5, 'episode_frame_number': 664, 'frame_number': 664}\n",
      "3 {'lives': 5, 'episode_frame_number': 668, 'frame_number': 668}\n",
      "3 {'lives': 5, 'episode_frame_number': 672, 'frame_number': 672}\n",
      "3 {'lives': 5, 'episode_frame_number': 676, 'frame_number': 676}\n",
      "0 {'lives': 5, 'episode_frame_number': 680, 'frame_number': 680}\n",
      "3 {'lives': 5, 'episode_frame_number': 684, 'frame_number': 684}\n",
      "3 {'lives': 5, 'episode_frame_number': 688, 'frame_number': 688}\n",
      "1 {'lives': 5, 'episode_frame_number': 692, 'frame_number': 692}\n",
      "3 {'lives': 5, 'episode_frame_number': 696, 'frame_number': 696}\n",
      "3 {'lives': 5, 'episode_frame_number': 700, 'frame_number': 700}\n",
      "3 {'lives': 5, 'episode_frame_number': 704, 'frame_number': 704}\n",
      "3 {'lives': 5, 'episode_frame_number': 708, 'frame_number': 708}\n",
      "3 {'lives': 4, 'episode_frame_number': 712, 'frame_number': 712}\n",
      "3 {'lives': 4, 'episode_frame_number': 716, 'frame_number': 716}\n",
      "3 {'lives': 4, 'episode_frame_number': 720, 'frame_number': 720}\n",
      "3 {'lives': 4, 'episode_frame_number': 724, 'frame_number': 724}\n",
      "1 {'lives': 4, 'episode_frame_number': 728, 'frame_number': 728}\n",
      "3 {'lives': 4, 'episode_frame_number': 732, 'frame_number': 732}\n",
      "3 {'lives': 4, 'episode_frame_number': 736, 'frame_number': 736}\n",
      "3 {'lives': 4, 'episode_frame_number': 740, 'frame_number': 740}\n",
      "3 {'lives': 4, 'episode_frame_number': 744, 'frame_number': 744}\n",
      "3 {'lives': 4, 'episode_frame_number': 748, 'frame_number': 748}\n",
      "3 {'lives': 4, 'episode_frame_number': 752, 'frame_number': 752}\n",
      "3 {'lives': 4, 'episode_frame_number': 756, 'frame_number': 756}\n",
      "3 {'lives': 4, 'episode_frame_number': 760, 'frame_number': 760}\n",
      "3 {'lives': 4, 'episode_frame_number': 764, 'frame_number': 764}\n",
      "3 {'lives': 4, 'episode_frame_number': 768, 'frame_number': 768}\n",
      "3 {'lives': 4, 'episode_frame_number': 772, 'frame_number': 772}\n",
      "3 {'lives': 4, 'episode_frame_number': 776, 'frame_number': 776}\n",
      "3 {'lives': 4, 'episode_frame_number': 780, 'frame_number': 780}\n",
      "3 {'lives': 4, 'episode_frame_number': 784, 'frame_number': 784}\n",
      "3 {'lives': 4, 'episode_frame_number': 788, 'frame_number': 788}\n",
      "3 {'lives': 4, 'episode_frame_number': 792, 'frame_number': 792}\n",
      "3 {'lives': 4, 'episode_frame_number': 796, 'frame_number': 796}\n",
      "3 {'lives': 4, 'episode_frame_number': 800, 'frame_number': 800}\n",
      "3 {'lives': 4, 'episode_frame_number': 804, 'frame_number': 804}\n",
      "3 {'lives': 4, 'episode_frame_number': 808, 'frame_number': 808}\n",
      "3 {'lives': 4, 'episode_frame_number': 812, 'frame_number': 812}\n",
      "3 {'lives': 4, 'episode_frame_number': 816, 'frame_number': 816}\n",
      "3 {'lives': 4, 'episode_frame_number': 820, 'frame_number': 820}\n",
      "3 {'lives': 4, 'episode_frame_number': 824, 'frame_number': 824}\n",
      "3 {'lives': 4, 'episode_frame_number': 828, 'frame_number': 828}\n",
      "3 {'lives': 4, 'episode_frame_number': 832, 'frame_number': 832}\n",
      "3 {'lives': 4, 'episode_frame_number': 836, 'frame_number': 836}\n",
      "3 {'lives': 4, 'episode_frame_number': 840, 'frame_number': 840}\n",
      "3 {'lives': 4, 'episode_frame_number': 844, 'frame_number': 844}\n",
      "3 {'lives': 4, 'episode_frame_number': 848, 'frame_number': 848}\n",
      "3 {'lives': 4, 'episode_frame_number': 852, 'frame_number': 852}\n",
      "3 {'lives': 4, 'episode_frame_number': 856, 'frame_number': 856}\n",
      "3 {'lives': 4, 'episode_frame_number': 860, 'frame_number': 860}\n",
      "1 {'lives': 4, 'episode_frame_number': 864, 'frame_number': 864}\n",
      "3 {'lives': 4, 'episode_frame_number': 868, 'frame_number': 868}\n",
      "3 {'lives': 4, 'episode_frame_number': 872, 'frame_number': 872}\n",
      "3 {'lives': 4, 'episode_frame_number': 876, 'frame_number': 876}\n",
      "3 {'lives': 4, 'episode_frame_number': 880, 'frame_number': 880}\n",
      "3 {'lives': 4, 'episode_frame_number': 884, 'frame_number': 884}\n",
      "3 {'lives': 4, 'episode_frame_number': 888, 'frame_number': 888}\n",
      "3 {'lives': 4, 'episode_frame_number': 892, 'frame_number': 892}\n",
      "3 {'lives': 4, 'episode_frame_number': 896, 'frame_number': 896}\n",
      "2 {'lives': 4, 'episode_frame_number': 900, 'frame_number': 900}\n",
      "3 {'lives': 4, 'episode_frame_number': 904, 'frame_number': 904}\n",
      "3 {'lives': 4, 'episode_frame_number': 908, 'frame_number': 908}\n",
      "3 {'lives': 4, 'episode_frame_number': 912, 'frame_number': 912}\n",
      "3 {'lives': 4, 'episode_frame_number': 916, 'frame_number': 916}\n",
      "3 {'lives': 4, 'episode_frame_number': 920, 'frame_number': 920}\n",
      "3 {'lives': 4, 'episode_frame_number': 924, 'frame_number': 924}\n",
      "3 {'lives': 4, 'episode_frame_number': 928, 'frame_number': 928}\n",
      "3 {'lives': 4, 'episode_frame_number': 932, 'frame_number': 932}\n",
      "3 {'lives': 4, 'episode_frame_number': 936, 'frame_number': 936}\n",
      "3 {'lives': 4, 'episode_frame_number': 940, 'frame_number': 940}\n",
      "3 {'lives': 3, 'episode_frame_number': 944, 'frame_number': 944}\n",
      "3 {'lives': 3, 'episode_frame_number': 948, 'frame_number': 948}\n",
      "3 {'lives': 3, 'episode_frame_number': 952, 'frame_number': 952}\n",
      "3 {'lives': 3, 'episode_frame_number': 956, 'frame_number': 956}\n",
      "3 {'lives': 3, 'episode_frame_number': 960, 'frame_number': 960}\n",
      "3 {'lives': 3, 'episode_frame_number': 964, 'frame_number': 964}\n",
      "3 {'lives': 3, 'episode_frame_number': 968, 'frame_number': 968}\n",
      "3 {'lives': 3, 'episode_frame_number': 972, 'frame_number': 972}\n",
      "3 {'lives': 3, 'episode_frame_number': 976, 'frame_number': 976}\n",
      "3 {'lives': 3, 'episode_frame_number': 980, 'frame_number': 980}\n",
      "3 {'lives': 3, 'episode_frame_number': 984, 'frame_number': 984}\n",
      "3 {'lives': 3, 'episode_frame_number': 988, 'frame_number': 988}\n",
      "3 {'lives': 3, 'episode_frame_number': 992, 'frame_number': 992}\n",
      "3 {'lives': 3, 'episode_frame_number': 996, 'frame_number': 996}\n",
      "3 {'lives': 3, 'episode_frame_number': 1000, 'frame_number': 1000}\n",
      "3 {'lives': 3, 'episode_frame_number': 1004, 'frame_number': 1004}\n",
      "3 {'lives': 3, 'episode_frame_number': 1008, 'frame_number': 1008}\n",
      "3 {'lives': 3, 'episode_frame_number': 1012, 'frame_number': 1012}\n",
      "3 {'lives': 3, 'episode_frame_number': 1016, 'frame_number': 1016}\n",
      "2 {'lives': 3, 'episode_frame_number': 1020, 'frame_number': 1020}\n",
      "3 {'lives': 3, 'episode_frame_number': 1024, 'frame_number': 1024}\n",
      "0 {'lives': 3, 'episode_frame_number': 1028, 'frame_number': 1028}\n",
      "3 {'lives': 3, 'episode_frame_number': 1032, 'frame_number': 1032}\n",
      "3 {'lives': 3, 'episode_frame_number': 1036, 'frame_number': 1036}\n",
      "3 {'lives': 3, 'episode_frame_number': 1040, 'frame_number': 1040}\n",
      "3 {'lives': 3, 'episode_frame_number': 1044, 'frame_number': 1044}\n",
      "3 {'lives': 3, 'episode_frame_number': 1048, 'frame_number': 1048}\n",
      "3 {'lives': 3, 'episode_frame_number': 1052, 'frame_number': 1052}\n",
      "3 {'lives': 3, 'episode_frame_number': 1056, 'frame_number': 1056}\n",
      "3 {'lives': 3, 'episode_frame_number': 1060, 'frame_number': 1060}\n",
      "3 {'lives': 3, 'episode_frame_number': 1064, 'frame_number': 1064}\n",
      "3 {'lives': 3, 'episode_frame_number': 1068, 'frame_number': 1068}\n",
      "3 {'lives': 3, 'episode_frame_number': 1072, 'frame_number': 1072}\n",
      "3 {'lives': 3, 'episode_frame_number': 1076, 'frame_number': 1076}\n",
      "3 {'lives': 3, 'episode_frame_number': 1080, 'frame_number': 1080}\n",
      "3 {'lives': 3, 'episode_frame_number': 1084, 'frame_number': 1084}\n",
      "3 {'lives': 3, 'episode_frame_number': 1088, 'frame_number': 1088}\n",
      "3 {'lives': 3, 'episode_frame_number': 1092, 'frame_number': 1092}\n",
      "3 {'lives': 3, 'episode_frame_number': 1096, 'frame_number': 1096}\n",
      "3 {'lives': 3, 'episode_frame_number': 1100, 'frame_number': 1100}\n",
      "3 {'lives': 3, 'episode_frame_number': 1104, 'frame_number': 1104}\n",
      "3 {'lives': 3, 'episode_frame_number': 1108, 'frame_number': 1108}\n",
      "3 {'lives': 3, 'episode_frame_number': 1112, 'frame_number': 1112}\n",
      "3 {'lives': 3, 'episode_frame_number': 1116, 'frame_number': 1116}\n",
      "2 {'lives': 3, 'episode_frame_number': 1120, 'frame_number': 1120}\n",
      "3 {'lives': 3, 'episode_frame_number': 1124, 'frame_number': 1124}\n",
      "3 {'lives': 3, 'episode_frame_number': 1128, 'frame_number': 1128}\n",
      "3 {'lives': 3, 'episode_frame_number': 1132, 'frame_number': 1132}\n",
      "3 {'lives': 3, 'episode_frame_number': 1136, 'frame_number': 1136}\n",
      "2 {'lives': 3, 'episode_frame_number': 1140, 'frame_number': 1140}\n",
      "3 {'lives': 3, 'episode_frame_number': 1144, 'frame_number': 1144}\n",
      "3 {'lives': 3, 'episode_frame_number': 1148, 'frame_number': 1148}\n",
      "3 {'lives': 3, 'episode_frame_number': 1152, 'frame_number': 1152}\n",
      "3 {'lives': 3, 'episode_frame_number': 1156, 'frame_number': 1156}\n",
      "0 {'lives': 3, 'episode_frame_number': 1160, 'frame_number': 1160}\n",
      "3 {'lives': 3, 'episode_frame_number': 1164, 'frame_number': 1164}\n",
      "1 {'lives': 3, 'episode_frame_number': 1168, 'frame_number': 1168}\n",
      "3 {'lives': 3, 'episode_frame_number': 1172, 'frame_number': 1172}\n",
      "3 {'lives': 3, 'episode_frame_number': 1176, 'frame_number': 1176}\n",
      "3 {'lives': 3, 'episode_frame_number': 1180, 'frame_number': 1180}\n",
      "0 {'lives': 3, 'episode_frame_number': 1184, 'frame_number': 1184}\n",
      "3 {'lives': 3, 'episode_frame_number': 1188, 'frame_number': 1188}\n",
      "3 {'lives': 3, 'episode_frame_number': 1192, 'frame_number': 1192}\n",
      "3 {'lives': 3, 'episode_frame_number': 1196, 'frame_number': 1196}\n",
      "3 {'lives': 3, 'episode_frame_number': 1200, 'frame_number': 1200}\n",
      "3 {'lives': 3, 'episode_frame_number': 1204, 'frame_number': 1204}\n",
      "3 {'lives': 3, 'episode_frame_number': 1208, 'frame_number': 1208}\n",
      "3 {'lives': 3, 'episode_frame_number': 1212, 'frame_number': 1212}\n",
      "3 {'lives': 3, 'episode_frame_number': 1216, 'frame_number': 1216}\n",
      "3 {'lives': 3, 'episode_frame_number': 1220, 'frame_number': 1220}\n",
      "3 {'lives': 3, 'episode_frame_number': 1224, 'frame_number': 1224}\n",
      "3 {'lives': 3, 'episode_frame_number': 1228, 'frame_number': 1228}\n",
      "3 {'lives': 3, 'episode_frame_number': 1232, 'frame_number': 1232}\n",
      "3 {'lives': 3, 'episode_frame_number': 1236, 'frame_number': 1236}\n",
      "3 {'lives': 3, 'episode_frame_number': 1240, 'frame_number': 1240}\n",
      "2 {'lives': 3, 'episode_frame_number': 1244, 'frame_number': 1244}\n",
      "3 {'lives': 3, 'episode_frame_number': 1248, 'frame_number': 1248}\n",
      "3 {'lives': 3, 'episode_frame_number': 1252, 'frame_number': 1252}\n",
      "3 {'lives': 3, 'episode_frame_number': 1256, 'frame_number': 1256}\n",
      "3 {'lives': 3, 'episode_frame_number': 1260, 'frame_number': 1260}\n",
      "3 {'lives': 2, 'episode_frame_number': 1264, 'frame_number': 1264}\n",
      "3 {'lives': 2, 'episode_frame_number': 1268, 'frame_number': 1268}\n",
      "0 {'lives': 2, 'episode_frame_number': 1272, 'frame_number': 1272}\n",
      "3 {'lives': 2, 'episode_frame_number': 1276, 'frame_number': 1276}\n",
      "3 {'lives': 2, 'episode_frame_number': 1280, 'frame_number': 1280}\n",
      "3 {'lives': 2, 'episode_frame_number': 1284, 'frame_number': 1284}\n",
      "3 {'lives': 2, 'episode_frame_number': 1288, 'frame_number': 1288}\n",
      "3 {'lives': 2, 'episode_frame_number': 1292, 'frame_number': 1292}\n",
      "3 {'lives': 2, 'episode_frame_number': 1296, 'frame_number': 1296}\n",
      "3 {'lives': 2, 'episode_frame_number': 1300, 'frame_number': 1300}\n",
      "3 {'lives': 2, 'episode_frame_number': 1304, 'frame_number': 1304}\n",
      "3 {'lives': 2, 'episode_frame_number': 1308, 'frame_number': 1308}\n",
      "3 {'lives': 2, 'episode_frame_number': 1312, 'frame_number': 1312}\n",
      "3 {'lives': 2, 'episode_frame_number': 1316, 'frame_number': 1316}\n",
      "3 {'lives': 2, 'episode_frame_number': 1320, 'frame_number': 1320}\n",
      "3 {'lives': 2, 'episode_frame_number': 1324, 'frame_number': 1324}\n",
      "3 {'lives': 2, 'episode_frame_number': 1328, 'frame_number': 1328}\n",
      "3 {'lives': 2, 'episode_frame_number': 1332, 'frame_number': 1332}\n",
      "3 {'lives': 2, 'episode_frame_number': 1336, 'frame_number': 1336}\n",
      "3 {'lives': 2, 'episode_frame_number': 1340, 'frame_number': 1340}\n",
      "3 {'lives': 2, 'episode_frame_number': 1344, 'frame_number': 1344}\n",
      "3 {'lives': 2, 'episode_frame_number': 1348, 'frame_number': 1348}\n",
      "3 {'lives': 2, 'episode_frame_number': 1352, 'frame_number': 1352}\n",
      "3 {'lives': 2, 'episode_frame_number': 1356, 'frame_number': 1356}\n",
      "3 {'lives': 2, 'episode_frame_number': 1360, 'frame_number': 1360}\n",
      "3 {'lives': 2, 'episode_frame_number': 1364, 'frame_number': 1364}\n",
      "3 {'lives': 2, 'episode_frame_number': 1368, 'frame_number': 1368}\n",
      "3 {'lives': 2, 'episode_frame_number': 1372, 'frame_number': 1372}\n",
      "2 {'lives': 2, 'episode_frame_number': 1376, 'frame_number': 1376}\n",
      "3 {'lives': 2, 'episode_frame_number': 1380, 'frame_number': 1380}\n",
      "3 {'lives': 2, 'episode_frame_number': 1384, 'frame_number': 1384}\n",
      "3 {'lives': 2, 'episode_frame_number': 1388, 'frame_number': 1388}\n",
      "1 {'lives': 2, 'episode_frame_number': 1392, 'frame_number': 1392}\n",
      "3 {'lives': 2, 'episode_frame_number': 1396, 'frame_number': 1396}\n",
      "3 {'lives': 2, 'episode_frame_number': 1400, 'frame_number': 1400}\n",
      "3 {'lives': 2, 'episode_frame_number': 1404, 'frame_number': 1404}\n",
      "3 {'lives': 2, 'episode_frame_number': 1408, 'frame_number': 1408}\n",
      "3 {'lives': 2, 'episode_frame_number': 1412, 'frame_number': 1412}\n",
      "3 {'lives': 2, 'episode_frame_number': 1416, 'frame_number': 1416}\n",
      "3 {'lives': 2, 'episode_frame_number': 1420, 'frame_number': 1420}\n",
      "3 {'lives': 2, 'episode_frame_number': 1424, 'frame_number': 1424}\n",
      "3 {'lives': 2, 'episode_frame_number': 1428, 'frame_number': 1428}\n",
      "3 {'lives': 2, 'episode_frame_number': 1432, 'frame_number': 1432}\n",
      "3 {'lives': 2, 'episode_frame_number': 1436, 'frame_number': 1436}\n",
      "0 {'lives': 2, 'episode_frame_number': 1440, 'frame_number': 1440}\n",
      "3 {'lives': 2, 'episode_frame_number': 1444, 'frame_number': 1444}\n",
      "3 {'lives': 2, 'episode_frame_number': 1448, 'frame_number': 1448}\n",
      "3 {'lives': 2, 'episode_frame_number': 1452, 'frame_number': 1452}\n",
      "3 {'lives': 2, 'episode_frame_number': 1456, 'frame_number': 1456}\n",
      "3 {'lives': 2, 'episode_frame_number': 1460, 'frame_number': 1460}\n",
      "3 {'lives': 2, 'episode_frame_number': 1464, 'frame_number': 1464}\n",
      "3 {'lives': 2, 'episode_frame_number': 1468, 'frame_number': 1468}\n",
      "3 {'lives': 2, 'episode_frame_number': 1472, 'frame_number': 1472}\n",
      "3 {'lives': 2, 'episode_frame_number': 1476, 'frame_number': 1476}\n",
      "3 {'lives': 2, 'episode_frame_number': 1480, 'frame_number': 1480}\n",
      "0 {'lives': 2, 'episode_frame_number': 1484, 'frame_number': 1484}\n",
      "3 {'lives': 2, 'episode_frame_number': 1488, 'frame_number': 1488}\n",
      "2 {'lives': 2, 'episode_frame_number': 1492, 'frame_number': 1492}\n",
      "3 {'lives': 2, 'episode_frame_number': 1496, 'frame_number': 1496}\n",
      "3 {'lives': 2, 'episode_frame_number': 1500, 'frame_number': 1500}\n",
      "3 {'lives': 2, 'episode_frame_number': 1504, 'frame_number': 1504}\n",
      "3 {'lives': 2, 'episode_frame_number': 1508, 'frame_number': 1508}\n",
      "3 {'lives': 2, 'episode_frame_number': 1512, 'frame_number': 1512}\n",
      "3 {'lives': 2, 'episode_frame_number': 1516, 'frame_number': 1516}\n",
      "0 {'lives': 2, 'episode_frame_number': 1520, 'frame_number': 1520}\n",
      "3 {'lives': 2, 'episode_frame_number': 1524, 'frame_number': 1524}\n",
      "3 {'lives': 2, 'episode_frame_number': 1528, 'frame_number': 1528}\n",
      "3 {'lives': 2, 'episode_frame_number': 1532, 'frame_number': 1532}\n",
      "1 {'lives': 2, 'episode_frame_number': 1536, 'frame_number': 1536}\n",
      "3 {'lives': 2, 'episode_frame_number': 1540, 'frame_number': 1540}\n",
      "3 {'lives': 2, 'episode_frame_number': 1544, 'frame_number': 1544}\n",
      "3 {'lives': 2, 'episode_frame_number': 1548, 'frame_number': 1548}\n",
      "3 {'lives': 2, 'episode_frame_number': 1552, 'frame_number': 1552}\n",
      "3 {'lives': 2, 'episode_frame_number': 1556, 'frame_number': 1556}\n",
      "3 {'lives': 2, 'episode_frame_number': 1560, 'frame_number': 1560}\n",
      "3 {'lives': 2, 'episode_frame_number': 1564, 'frame_number': 1564}\n",
      "3 {'lives': 2, 'episode_frame_number': 1568, 'frame_number': 1568}\n",
      "0 {'lives': 2, 'episode_frame_number': 1572, 'frame_number': 1572}\n",
      "3 {'lives': 2, 'episode_frame_number': 1576, 'frame_number': 1576}\n",
      "3 {'lives': 2, 'episode_frame_number': 1580, 'frame_number': 1580}\n",
      "3 {'lives': 2, 'episode_frame_number': 1584, 'frame_number': 1584}\n",
      "3 {'lives': 2, 'episode_frame_number': 1588, 'frame_number': 1588}\n",
      "3 {'lives': 2, 'episode_frame_number': 1592, 'frame_number': 1592}\n",
      "3 {'lives': 2, 'episode_frame_number': 1596, 'frame_number': 1596}\n",
      "3 {'lives': 2, 'episode_frame_number': 1600, 'frame_number': 1600}\n",
      "3 {'lives': 2, 'episode_frame_number': 1604, 'frame_number': 1604}\n",
      "0 {'lives': 2, 'episode_frame_number': 1608, 'frame_number': 1608}\n",
      "3 {'lives': 1, 'episode_frame_number': 1612, 'frame_number': 1612}\n",
      "3 {'lives': 1, 'episode_frame_number': 1616, 'frame_number': 1616}\n",
      "3 {'lives': 1, 'episode_frame_number': 1620, 'frame_number': 1620}\n",
      "3 {'lives': 1, 'episode_frame_number': 1624, 'frame_number': 1624}\n",
      "1 {'lives': 1, 'episode_frame_number': 1628, 'frame_number': 1628}\n",
      "3 {'lives': 1, 'episode_frame_number': 1632, 'frame_number': 1632}\n",
      "3 {'lives': 1, 'episode_frame_number': 1636, 'frame_number': 1636}\n",
      "3 {'lives': 1, 'episode_frame_number': 1640, 'frame_number': 1640}\n",
      "3 {'lives': 1, 'episode_frame_number': 1644, 'frame_number': 1644}\n",
      "3 {'lives': 1, 'episode_frame_number': 1648, 'frame_number': 1648}\n",
      "3 {'lives': 1, 'episode_frame_number': 1652, 'frame_number': 1652}\n",
      "3 {'lives': 1, 'episode_frame_number': 1656, 'frame_number': 1656}\n",
      "3 {'lives': 1, 'episode_frame_number': 1660, 'frame_number': 1660}\n",
      "3 {'lives': 1, 'episode_frame_number': 1664, 'frame_number': 1664}\n",
      "3 {'lives': 1, 'episode_frame_number': 1668, 'frame_number': 1668}\n",
      "3 {'lives': 1, 'episode_frame_number': 1672, 'frame_number': 1672}\n",
      "3 {'lives': 1, 'episode_frame_number': 1676, 'frame_number': 1676}\n",
      "3 {'lives': 1, 'episode_frame_number': 1680, 'frame_number': 1680}\n",
      "3 {'lives': 1, 'episode_frame_number': 1684, 'frame_number': 1684}\n",
      "3 {'lives': 1, 'episode_frame_number': 1688, 'frame_number': 1688}\n",
      "3 {'lives': 1, 'episode_frame_number': 1692, 'frame_number': 1692}\n",
      "1 {'lives': 1, 'episode_frame_number': 1696, 'frame_number': 1696}\n",
      "3 {'lives': 1, 'episode_frame_number': 1700, 'frame_number': 1700}\n",
      "3 {'lives': 1, 'episode_frame_number': 1704, 'frame_number': 1704}\n",
      "3 {'lives': 1, 'episode_frame_number': 1708, 'frame_number': 1708}\n",
      "3 {'lives': 1, 'episode_frame_number': 1712, 'frame_number': 1712}\n",
      "3 {'lives': 1, 'episode_frame_number': 1716, 'frame_number': 1716}\n",
      "3 {'lives': 1, 'episode_frame_number': 1720, 'frame_number': 1720}\n",
      "3 {'lives': 1, 'episode_frame_number': 1724, 'frame_number': 1724}\n",
      "3 {'lives': 1, 'episode_frame_number': 1728, 'frame_number': 1728}\n",
      "1 {'lives': 1, 'episode_frame_number': 1732, 'frame_number': 1732}\n",
      "3 {'lives': 1, 'episode_frame_number': 1736, 'frame_number': 1736}\n",
      "3 {'lives': 1, 'episode_frame_number': 1740, 'frame_number': 1740}\n",
      "3 {'lives': 1, 'episode_frame_number': 1744, 'frame_number': 1744}\n",
      "3 {'lives': 1, 'episode_frame_number': 1748, 'frame_number': 1748}\n",
      "3 {'lives': 1, 'episode_frame_number': 1752, 'frame_number': 1752}\n",
      "3 {'lives': 1, 'episode_frame_number': 1756, 'frame_number': 1756}\n",
      "3 {'lives': 1, 'episode_frame_number': 1760, 'frame_number': 1760}\n",
      "3 {'lives': 1, 'episode_frame_number': 1764, 'frame_number': 1764}\n",
      "3 {'lives': 1, 'episode_frame_number': 1768, 'frame_number': 1768}\n",
      "3 {'lives': 1, 'episode_frame_number': 1772, 'frame_number': 1772}\n",
      "3 {'lives': 1, 'episode_frame_number': 1776, 'frame_number': 1776}\n",
      "3 {'lives': 1, 'episode_frame_number': 1780, 'frame_number': 1780}\n",
      "3 {'lives': 1, 'episode_frame_number': 1784, 'frame_number': 1784}\n",
      "3 {'lives': 1, 'episode_frame_number': 1788, 'frame_number': 1788}\n",
      "3 {'lives': 1, 'episode_frame_number': 1792, 'frame_number': 1792}\n",
      "3 {'lives': 1, 'episode_frame_number': 1796, 'frame_number': 1796}\n",
      "3 {'lives': 1, 'episode_frame_number': 1800, 'frame_number': 1800}\n",
      "3 {'lives': 1, 'episode_frame_number': 1804, 'frame_number': 1804}\n",
      "1 {'lives': 1, 'episode_frame_number': 1808, 'frame_number': 1808}\n",
      "3 {'lives': 1, 'episode_frame_number': 1812, 'frame_number': 1812}\n",
      "3 {'lives': 1, 'episode_frame_number': 1816, 'frame_number': 1816}\n",
      "3 {'lives': 1, 'episode_frame_number': 1820, 'frame_number': 1820}\n",
      "3 {'lives': 1, 'episode_frame_number': 1824, 'frame_number': 1824}\n",
      "3 {'lives': 1, 'episode_frame_number': 1828, 'frame_number': 1828}\n",
      "3 {'lives': 1, 'episode_frame_number': 1832, 'frame_number': 1832}\n",
      "3 {'lives': 1, 'episode_frame_number': 1836, 'frame_number': 1836}\n",
      "3 {'lives': 1, 'episode_frame_number': 1840, 'frame_number': 1840}\n",
      "3 {'lives': 1, 'episode_frame_number': 1844, 'frame_number': 1844}\n",
      "3 {'lives': 1, 'episode_frame_number': 1848, 'frame_number': 1848}\n",
      "3 {'lives': 0, 'episode_frame_number': 1851, 'frame_number': 1851}\n"
     ]
    }
   ],
   "source": [
    "s, info = reset(env)\n",
    "s = torch.tensor(s)\n",
    "terminated = False\n",
    "while terminated == False:\n",
    "    a = dqn.select_next_action(s.permute(2, 0, 1).unsqueeze(0), 0.1)\n",
    "    s_prime, r, terminated, truncated, info = step(env, a)\n",
    "\n",
    "    s_prime = preprocess(s_prime)\n",
    "    s_prime = torch.cat([s, s_prime.unsqueeze(2)], 2)[:, :, -4:]\n",
    "    print(a, info)\n",
    "    s = s_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602edb90",
   "metadata": {
    "id": "602edb90",
    "outputId": "c0bccd79-ad9f-4c94-b944-8440658e7cc5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.047"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(51 * 197) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f8044",
   "metadata": {
    "id": "9b9f8044",
    "outputId": "18c3694a-3b66-4b88-c3d7-c1a174a74017"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60244d69",
   "metadata": {
    "id": "60244d69"
   },
   "outputs": [],
   "source": [
    "y_hat, y = get_batch(dqn, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f030f44",
   "metadata": {
    "id": "0f030f44",
    "outputId": "d23ce108-2c0a-4363-e335-b8f01fa86f93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ee48f",
   "metadata": {
    "id": "751ee48f",
    "outputId": "d5187cad-cc51-4d9c-aeee-f5b5a150d199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772 µs ± 11.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "s_prime, r, terminated, truncated, info = step(env, a)\n",
    "s_prime = preprocess(s_prime)\n",
    "s_prime = torch.cat([s, s_prime.unsqueeze(2)], 2)[:, :, -4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b31313",
   "metadata": {
    "id": "50b31313",
    "outputId": "c49049a1-9015-4698-cf1f-81b50e9fc33c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.076"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "22 * 458 /1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc5a83",
   "metadata": {
    "id": "a8bc5a83"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
